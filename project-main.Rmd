---
title: "Housing Market Dynamics"
subtitle: "From PCA to Factor Models"
author: "Yiran Yao"
date: "2025-08-04"
output:
  html_document:
    css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      fig.align = "center",
                      fig.width = 8, 
                      fig.height = 4
                      )
```

```{r library}
library(tidyverse)
library(janitor)
library(lubridate)
library(forecast)
library(fabletools)
library(fpp3)
library(urca)
library(broom)
library(tseries)
library(scales)
library(slider)
library(kableExtra)
library(patchwork)
library(sf)
library(distributional) 
library(glue)
library(readxl)
library(GGally)
library(zoo)
library(strucchange)
#library(plotly)
```

```{r load-clean-data}
# Raw housing index 
city_indexes <- read_csv("data/city_indexes.csv")
sa4_indexes <- read_csv("data/indexes_city_and_sa4.csv")

# PC proxy with identified characteristic
factor_trends <- read_csv("data/df_factor_trends.csv")

# Static regression coef 
reg_coefs <- read_csv("data/df_reg_coefs.csv") 

# PC score and loadings 
pcs <- read_csv("data/df_pcs.csv")
eofs_city <- read_csv("data/df_eofs_city.csv")
#eofs_sa2 <- read_csv("data/df_eofs.csv")
eofs_sa4 <- read_csv("data/df_eofs_sa4.csv")


# Economic data - need cleaning 
#gdp_mining <- read_xlsx("data/data.xlsx")
#exchange_rate <- read_xlsx("data/key_economic_data.xlsx")
#mining_expenditure <- read_xlsx("data/mining_expenditure.xlsx", sheet = 2)

# For plotting SA4 geo 
sa4 <- st_read("data/SA4_2021_AUST_SHP_GDA2020/SA4_2021_AUST_GDA2020.shp", quiet = TRUE)

# Cleaning names 
city_indexes <- city_indexes |> 
  clean_names() 

reg_coefs <- reg_coefs |>
  filter(!(region == "AUSTRALIAN CAPITAL TERRITORY" & duplicated(region)))
```

```{r compute-mining-ps}
# Compute the Perth Sydney Spread 
PS_raw <- city_indexes |> 
  select(month_date, greater_sydney, greater_perth)

U_raw <- factor_trends |> 
  select(month_date, market) 

com_PS <- PS_raw |> inner_join(U_raw, by = "month_date")

# Extract the numeric values and compute alpha 
mu_perth   <- com_PS$`greater_perth`
mu_sydney  <- com_PS$`greater_sydney`
U          <- com_PS$market   

alpha <- cov(mu_perth, U, use = "complete.obs") /
         cov(mu_sydney, U, use = "complete.obs")

# Add in PS spread to factor_trend as mining_ps 
com_PS <- com_PS |> 
  mutate(mining_ps = `greater_perth` - alpha * `greater_sydney`) |> 
  select(month_date, mining_ps)

factor_trends <- factor_trends |> 
  inner_join(com_PS, by = "month_date")
```

```{r pc3-proxy-current, eval = FALSE}
# Prepare the data we need - pc loadings, pc scores, lifestyle trend 
pc3_pxy <- eofs_city |> 
  filter(mode == 2) |> 
  arrange(loading_value)

Z3 <- pcs |> 
  filter(mode == 2) |>  
  select(period, pc_value)

# Melbourne – Rest of NSW
MN_raw <- city_indexes |> 
  select(month_date, rest_of_nsw, greater_melbourne)

U_raw <- factor_trends |> 
  select(month_date, market) 

com_MN <- MN_raw |> inner_join(U_raw, by = "month_date")

mu_rest_nsw   <- com_MN$`rest_of_nsw`
mu_melbourne  <- com_MN$`greater_melbourne`
U          <- com_MN$market 

alpha1 <- cov(mu_rest_nsw, U, use = "complete.obs") /
         cov(mu_melbourne, U, use = "complete.obs")

com_MN <- com_MN |> 
  mutate(lifestyle_1 = `rest_of_nsw` - alpha1 * `greater_melbourne`) |> 
  select(month_date, lifestyle_1)

factor_trends_1 <- factor_trends |> 
  inner_join(com_MQ, by = "month_date")

lifestyle_cmp <- factor_trends_1 |> 
  select(month_date, lifestyle, lifestyle_1) |>
  rename(period = month_date) |> 
  inner_join(Z3, by = "period") |> 
  mutate(
    z3_std     = as.numeric(scale(pc_value)),
    lifestyleorg_std = as.numeric(scale(lifestyle)),
    lifestyle1_std = as.numeric(scale(lifestyle_1))
  )

lifestyle_cmp |> 
  select(period, z3_std, lifestyleorg_std, lifestyle1_std ) |> 
  pivot_longer(-period, names_to = "series", values_to = "value") |> 
  mutate(series = recode(series,
                         z3_std = "PC3",
                         lifestyleorg_std = "δlifestyle",
                         lifestyle1_std = "nsw_mel")) |> 
  ggplot(aes(x = as.Date(period), y = value, colour = series)) +
  geom_line() +
  labs(x = "Period", y = "Standardised value", colour = "Series",
       title = "Current ") +
  theme_minimal()
```

```{r pc3-proxy-new, eval = FALSE}
Z3 <- pcs |> 
  filter(mode == 2) |>  
  select(period, pc_value)

pc3_pxy_sa4 <- eofs_sa4 |> 
  filter(mode == 2) |> 
  arrange(loading_value)

sa4_raw <- sa4_indexes |>
  select(
    month_date,
    # NEGATIVE basket (urban)
    `MELBOURNE - INNER`, `MELBOURNE - INNER EAST`, `MELBOURNE - INNER SOUTH`,
    `MELBOURNE - SOUTH EAST`, `MELBOURNE - NORTH EAST`, `MELBOURNE - NORTH WEST`,
    `MELBOURNE - OUTER EAST`, `MELBOURNE - WEST`,
    `SYDNEY - CITY AND INNER SOUTH`, `SYDNEY - INNER WEST`, `SYDNEY - INNER SOUTH WEST`,
    `SYDNEY - RYDE`, `SYDNEY - PARRAMATTA`,
    # POSITIVE basket (coastal)
    `SUNSHINE COAST`, `GOLD COAST`, `MID NORTH COAST`, `RICHMOND - TWEED`,
    `COFFS HARBOUR - GRAFTON`, `CENTRAL COAST`, `WIDE BAY`,
    `HUNTER VALLEY EXC NEWCASTLE`, `ILLAWARRA`, `SOUTHERN HIGHLANDS AND SHOALHAVEN`
  )

com_pc3 <- sa4_raw |> inner_join(U_raw, by = "month_date")

mu_positive <- rowMeans(com_pc3[, c(
  "SUNSHINE COAST",
  "GOLD COAST",
  "MID NORTH COAST",
  "RICHMOND - TWEED",
  "CENTRAL COAST",
  "WIDE BAY",
  "HUNTER VALLEY EXC NEWCASTLE",
  "COFFS HARBOUR - GRAFTON",
  "ILLAWARRA",
  "SOUTHERN HIGHLANDS AND SHOALHAVEN"
)])

mu_negative <- rowMeans(com_pc3[, c(
  "MELBOURNE - INNER",
  "MELBOURNE - INNER EAST",
  "MELBOURNE - INNER SOUTH",
  "MELBOURNE - SOUTH EAST",
  "SYDNEY - CITY AND INNER SOUTH",
  "SYDNEY - INNER WEST",
  "SYDNEY - INNER SOUTH WEST",
  "SYDNEY - PARRAMATTA"
)])
U          <- com_pc3$market 

alpha2 <- cov(mu_positive, U, use = "complete.obs") /
         cov(mu_negative, U, use = "complete.obs")

com_pc3 <- com_pc3 |>
  mutate(lifestyle_proxy = mu_positive - alpha2 * mu_negative)
# mutate(lifestyle_proxy = mu_positive - mu_negative)

factor_trends_2 <- factor_trends |> 
  inner_join(com_pc3, by = "month_date")

lifestyle_cmp <- factor_trends_2 |> 
  select(month_date, lifestyle, lifestyle_proxy) |>
  rename(period = month_date) |> 
  inner_join(Z3, by = "period") |> 
  mutate(
    z3_std     = as.numeric(scale(pc_value)),
    lifestyleorg_std = as.numeric(scale(lifestyle)),
    lifestyleproxy_std = as.numeric(scale(lifestyle_proxy))
  )

lifestyle_cmp |> 
  select(period, z3_std, lifestyleproxy_std) |> 
  pivot_longer(-period, names_to = "series", values_to = "value") |> 
  mutate(series = recode(series,
                         z3_std = "PC3",
                       #  lifestyleorg_std = "lifestyle original",
                         lifestyleproxy_std = "Lifestyle proxy")) |> 
  ggplot(aes(x = as.Date(period), y = value, colour = series)) +
  geom_line() +
  labs(x = "Period", y = "Standardised value", colour = "Series",
       title = "Current ") +
  theme_minimal()


```

# Introduction 

Traditional approaches to analysing housing indexes, such as hedonic or repeat-sales models, can introduce selection bias or violate underlying assumptions. Moreover, they fail to capture key regional, social, or macroeconomic factors that play a significant role in driving housing price movements.

In the recent research Sijp et al. (2025) propose a new approach that applies Principal Component Analysis (PCA) to regional SA4 level housing indexes, enabling the extraction of key drivers of price movements and offering a deeper understanding of the forces shaping housing markets.

The PCA results show that the first three principal components capture much of the behaviour of local price indexes, as a linear combination of their time series explains most of the variance in the housing index. Nevertheless, the PCA-based linear model lacks robustness across different time windows, and its coefficient estimates must be derived directly from the PCA procedure.

Our exploratory research extends the PCA approach by addressing these limitations through a factor linear model, which uses time series data directly, with independent variables serving as proxies for the PCA-derived principal components.

# Moving from PCA to Factor Model 

The national trend (U) exhibits a close correspondence with PC1 (market), while the Perth–Sydney spread (δPS) aligns with PC2 (mining). Given that these principal components account for the majority of variance in the index, our preliminary analysis focuses on U and δPS, with subsequent extensions planned. (WIP - might add lifestyle)

-   Explain on more of why did we move on from PCA to factor model
-   Explain how did we find the factors (like how did we take national as PC1, PS as PC2...)

We examined whether PC2 is better represented by the Brisbane–Sydney or Perth–Sydney spread by comparing the shapes of the standardised series and testing their correlations with PC2. The Perth–Sydney spread aligns more closely with PC2: The scatterplot shows a distribution that falls more evenly along the regression line, and its correlation with PC2 is slightly higher (98% versus 96% for the Brisbane–Sydney spread). While the difference is minor and not statistically significant, the Perth–Sydney spread is the stronger candidate as a proxy for PC2.

```{r examine-pc-corr}
# Join PC2 with both series 
Z2 <- pcs |> 
  filter(mode == 1) |>  
  select(period, pc_value)

mining_cmp <- factor_trends |> 
  select(month_date, mining, mining_ps) |>
  rename(period = month_date) |> 
  inner_join(Z2, by = "period") |> 
  mutate(
    z2_std     = as.numeric(scale(pc_value)),
    deltaBS_std = as.numeric(scale(mining)),
    deltaPS_std = as.numeric(scale(mining_ps))
  )

# Plot PC2 with both series 
plot_ts <- mining_cmp |> 
  select(period, z2_std, deltaBS_std, deltaPS_std) |> 
  pivot_longer(-period, names_to = "series", values_to = "value") |> 
  mutate(series = recode(series,
                         z2_std = "PC2",
                         deltaBS_std = "δBS",
                         deltaPS_std = "δPS")) |> 
  ggplot(aes(x = as.Date(period), y = value, colour = series)) +
  geom_line() +
  labs(x = "Period", y = "Standardised value", colour = "Series",
       title = "PC2 vs δBS and δPS (standardised)") +
  theme_minimal()

# Compare correlation 
corr_bs <- cor(mining_cmp$pc_value, mining_cmp$mining,   use = "complete.obs")
corr_ps <- cor(mining_cmp$pc_value, mining_cmp$mining_ps, use = "complete.obs")

# Scatter plot 
scatter_df <- mining_cmp |> 
  select(pc_value, mining, mining_ps) |> 
  pivot_longer(cols = c(mining, mining_ps),
               names_to = "spread", values_to = "spread_value") |> 
  mutate(spread = recode(spread,
                         mining    = glue("δBS (r = {round(corr_bs, 3)})"),
                         mining_ps = glue("δPS (r = {round(corr_ps, 3)})")))

plot_scatter <- ggplot(scatter_df, aes(x = pc_value, y = spread_value)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ spread, scales = "free_y") +
  labs(title = "PC2 vs spreads", x = "PC2", y = "Spread value") +
  theme_minimal()

plot_scatter 
plot_ts
```

```{r cointegration-test, eval = FALSE}
# Build monthly series of levels
city <- city_indexes |>
  transmute(
    month = yearmonth(floor_date(as.Date(month_date), "month")),
    bris  = as.numeric(greater_brisbane),
    syd   = as.numeric(greater_sydney)
  ) |>
  as_tsibble(index = month) 

# Unit-root checks
adf_bris_lvl  <- ur.df(city$bris, type = "trend", selectlags = "AIC")
adf_syd_lvl   <- ur.df(city$syd,  type = "trend", selectlags = "AIC")
adf_bris_diff <- ur.df(diff(city$bris), type = "none", selectlags = "AIC")
adf_syd_diff  <- ur.df(diff(city$syd),  type = "none", selectlags = "AIC")

# Long-run regression
cn_lm <- lm(bris ~ syd, data = as.data.frame(city))  
alpha <- coef(cn_lm)[2]
ect   <- resid(cn_lm)  

# ADF on residuals 
adf_ect <- ur.df(ect, type = "none", selectlags = "AIC")
summary(adf_ect)
```

## Data Source and Limitaions (WIP)

# Exploratory Analysis 

The exploratory analysis utilises visualisations to characterise the dynamics of the national trend (U) and the Perth–Sydney spread (δPS), thereby clarifying their underlying properties.

```{r autoplot}
# Transform df to tsibble
factor_trend_ts <- factor_trends |>
  transmute(
    month_date = yearmonth(as.Date(month_date)),
    U = as.numeric(market),
    PS = as.numeric(mining_ps)
  ) |>
  as_tsibble(index = month_date)

# Time-series autoplot
p1 <- factor_trend_ts |> autoplot(U) + labs(title = "U: National Trend", x = "Month", y = "Index")
p2 <- factor_trend_ts |> autoplot(PS) + labs(title = "δPS: Perth–Sydney Spread", x = "Month", y = "Index")

p1 + p2 
```

**Time-series plots:** 

* U rises steadily from 2003 to 2012, transitions into a slower growth phase until 2019, then undergoes a sharp regime shift during 2020–2022 (the COVID period) followed by persistently elevated and volatile levels, with growth rates remaining time-varying throughout.

* δPS displays a prolonged, uneven rise from 2003 to 2012, followed by a sharp regime shift, then COVID-related fluctuations, with the level adjusting and showing time-varying patterns.

```{r season-plot}
# Seasonal plots
p3  <- factor_trend_ts |> gg_season(U) + labs(title = "Seasonal Plot: U", x = "Month", y = "Index")
p4 <- factor_trend_ts |> gg_season(PS) + labs(title = "Seasonal Plot: δPS", x = "Month", y = "Index")
p3 + p4
```

**Seasonal plots:**

* U has apparent within-year slopes being dominated by the strong upward trend; no systematic month-of-year effects.

* δPS shows little evidence of month-to-month seasonality, with no particular month standing out, though its overall level varies between years.

```{r subseries-plot}
# Seasonal subseries
p5 <- factor_trend_ts |> gg_subseries(U) + labs(title = "Seasonal Subseries: U", x = "Month", y = "Index")
p6 <- factor_trend_ts |> gg_subseries(PS) + labs(title = "Seasonal Subseries: δPS", x = "Month", y = "Index")
p5 + p6
```

**Seasonal sub-series plot:** 

* Each monthly panel of U shows a steady upward rise with mean value across months remaining similar, reinforcing that the series is driven more by trend than by seasonality.

* The δPS seasonal sub-series plot reinforces the lack of a dominant seasonal pattern, as the monthly profiles are similar in shape and the corresponding means remain relatively flat and close to zero.


# Forecasting with the Factor Model 

With the factors now established, we proceed to fit time series forecasting models to each series individually. As a first step, we examine the series in more detail. STL decomposition plots allow us to closely inspect the trend, seasonality, and remainder components, which helps determine the most suitable forecasting models for the series’ characteristics.

Stationary tests (ADF and KPSS) indicate that both the national index (U) and the Perth–Sydney spread (PS) are non-stationary, so differencing is required. However, based on economic intuition that the PS series may be mean-reverting, we applied the Zivot–Andrews (ur.za) test. This confirmed that PS could be stationary with several structural break.

The dataset was partitioned into training and testing subsets, with one year of observations allocated to the testing set. Given that the data are recorded at a monthly frequency and span only 25 years, this allocation represents an optimal balance between ensuring sufficient data for model training while preserving an adequate sample for evaluation.

```{r trans-ts}
# Transform df to tsibble 
factor_trends_ts <- factor_trends |>
  transmute(date = yearmonth(month_date),
            U  = market,
            PS = mining) |> 
  as_tsibble(index = date) 
```

```{r stationary-test, eval = FALSE}
# U is non stationary, I(1)
adf_U <- ur.df(factor_trends_ts$U, type="trend", lags=12)
kpss_U <- kpss.test(factor_trends_ts$U, null="Trend")

# PS also I(1)
adf_PS <- ur.df(factor_trends_ts$PS, type="drift", lags=12) 
kpss_PS <- kpss.test(factor_trends_ts$PS, null="Level")

# PS is stationary with a single structural break at position 53: 2011 Apr
za <- ur.za(factor_trends_ts$PS, model = "both") 
```

## National Trend 

The STL decomposition of the national trend U reveals a smooth and persistent upward trajectory in the trend component, while the seasonal component is of relatively minor magnitude, however the annual seasonality is strong and repeating. To stabilise variation in the seasonal component, a Box–Cox transformation was applied using the Guerrero method, which produced a λ value of 0.9. Given its proximity to one, the transformation had a negligible effect on the modelling outcome. The remainder component displays mostly small noise, with some bigger shocks.

```{r fitting-u-plot}
# Identify fits 
U_stl <- factor_trends_ts |> 
  model(stl = STL(U))

U_stl |> 
  components() |> 
  autoplot() +
  labs(title = "STL Decomposition of National Trend U")

ggtsdisplay(factor_trends_ts$U, main = "ACF and PACF with National Trend U")

# Split testing and training 
train_U <- factor_trends_ts |> 
  select(U, date) |> 
  filter(date < yearmonth("2023 Jan")) 

test_U <- factor_trends_ts |> 
  select(U, date) |> 
  filter(date >= yearmonth("2023 Jan")) 
```

The modelling strategy was guided by exploratory data analysis and the STL decomposition, which suggested a random walk with drift, `ARIMA(U ∼ pdq(0,1,0))`, as an appropriate baseline specification. 

To refine the model, we subsequently evaluated neighbouring specifications with alternative autoregressive and moving average terms, using AICc values and residual diagnostics as selection criteria. 

In the modelling process, we evaluated a range of specifications informed both by diagnostic checks and prior knowledge. Inspection of the residuals suggested some semi-annual variation, and Fourier terms with `K = 2` were added to account for potential higher-frequency seasonality. We also investigated seasonal extensions with `P = 1`, alongside piecewise specifications with knots aligned to major economic events: the Global Financial Crisis (2008), the end of the mining investment boom (2012), and the onset of the COVID-19 pandemic (2020). These variations were assessed with respect to residual behaviour, statistical significance, and their ability to capture shocks apparent in the remainder component.

Among the fitted models, two candidates emerged as particularly strong. The first, an ARIMA(2,1,1) with Fourier terms (K = 2) and drift. A second competitive specification combined Fourier terms (K = 2) with intervention dummies (step and pulse) and an ARIMA(2,1,1) structure. 

```{r fit-u-model}
# Function to add knots - improvement can be done by examining the mining expenditure data 
add_pw <- function(df) {
  df |> 
    mutate(
      ym = yearmonth(date),
      # steps
      step2008 = as.integer(date >= yearmonth("2008 Jan")), #step 
      step2012 = as.integer(date >= yearmonth("2012 Jan")),
      ramp2020 = pmax(0L, as.integer(yearmonth(date) - yearmonth("2020 Mar"))), #ramp 
      step2020_03  = as.integer(date >= yearmonth("2020 Mar")),
      pulse2020_04 = as.integer(date ==  yearmonth("2020 Apr")) #pulse 
    ) |> 
    select(-ym)
}

train_U <- train_U |>
  as_tsibble(index = date) |>
  add_pw()

# Fit models 
fit_U <- train_U |> 
  model(
    autoarima = ARIMA(U),
    rw_drift  = ARIMA(U ~ 1 + pdq(0,1,0)),
    arima211  = ARIMA(U ~ 1 + fourier(K = 2) + pdq(2,1,1)),
    arima110  = ARIMA(U ~ 1 + fourier(K = 2) + pdq(1,1,0)),
    arima111  = ARIMA(U ~ 1 + fourier(K = 2) + pdq(1,1,1)),
    sarima211 = ARIMA(U ~ 1 + pdq(2,1,1) + PDQ(1,0,0)),
    piece = ARIMA(U ~ 1 + pdq(1,1,1) + step2008 + step2012 + ramp2020),
    step = ARIMA(U ~ fourier(K = 2) + step2020_03 + pulse2020_04 + pdq(2,1,1))
  ) 

# Examine fit  
glance(fit_U) |> arrange(AICc)

fit_U |> 
  select(arima211) |>
  gg_tsresiduals()

fit_U |> 
  select(arima211) |>
  gg_tsresiduals()
```

Although both models fall short of strictly meeting the residual whiteness criterion, particularly since the degrees of freedom were not clearly defined in the test results, we use them here mainly for relative comparison of performance. Visual inspection of the residual plots suggests that the residuals are approximately normally distributed, with only a small share of autocorrelation coefficients exceeding the significance bounds in the ACF plot. As a general rule of thumb, if fewer than about 5% of the lags lie outside these bounds, the residuals can reasonably be treated as white noise.

```{r fit-u-model-residual-test}
# Ljung-Box test - without defining dof - not a sensitive test  
lb_tbl <- fit_U |>
  augment() |> 
  features(.innov, ljung_box, lag = 12) |>
  select(.model, lb_stat, lb_pvalue) |> 
  arrange(desc(lb_pvalue)) |> 
  print()
```

## Mining Trend 

The mining trend (PS) oscillates around zero and appears mean-reverting with no evidence ofpermanent trend. Seasonality is present but relatively stable across years, while longer multi-year swings suggest the need for more flexible seasonal modelling. The residuals still display structure, with a clear AR(1) cut-off rather than pure white noise. 

```{r fit-ps-plot}
PS_stl <- factor_trends_ts |> 
  model(stl = STL(PS))

PS_stl |> 
  components() |> 
  autoplot() +
  labs(title = "STL Decomposition of Mining Trend PS")

ggtsdisplay(factor_trends_ts$PS, main = "ACF and PACF with Mining Trend PS")

# Split training and testing set 
train_PS <- factor_trends_ts |>
  select(date, PS) |>
  filter(date < yearmonth("2023 Jan")) |>
  as_tsibble(index = date)

test_PS <- factor_trends_ts |> 
  select(date, PS) |>
  filter(date >= yearmonth("2023 Jan")) |>
  as_tsibble(index = date)
```

The mining trend (PS) captures the dynamics of the resource boom, which we interpret as a one-off structural break rather than a permanent driver. Prices surged sharply during the 2000s mining boom, collapsed around 2012, and have remained stagnant since. If this raw factor is carried forward into forecasts, the boom–bust cycle dominates and produces unstable predictions with excessively wide intervals.

To address this, we treat the boom as a temporary structural shock. We introduce a dummy variable (set to 1 during the boom/decline period and 0 otherwise) when modelling PS, and then forecast only the residual stationary component. This approach effectively assumes that another boom of comparable scale is unlikely.

The boom and decline phases were identified using the `breakpoints` function from the `strucchange` package, which detected three structural breaks in the series. The segment containing the global peak was classified as the “boom” period, followed by the “decline” period. These periods were then encoded into dummy variables for use in the modelling process.

```{r detect-ps-break}
ps_df <- train_PS |> 
  as_tibble() |> 
  transmute(
    date = as.Date(date),    
    PS = as.numeric(PS)   
  ) |> 
  arrange(date)

# Partition the series into boom and decline windows based on structural breaks
ps_zoo <- zoo(ps_df$PS, order.by = ps_df$date)
y  <- coredata(ps_zoo)
ti <- index(ps_zoo)
n  <- length(y)

# Three trend breaks 
bp_tr <- breakpoints(ps_zoo ~ time(ps_zoo), breaks = 3)

# Keep the breaks that actually exist 
idx <- bp_tr$breakpoints
idx <- idx[!is.na(idx)]

# Segment boundaries
cuts <- c(0, idx, n) 

# Locate the global peak and the segment that contains it
peak_i <- which.max(y)
seg_id <- findInterval(peak_i, cuts) 

# Boom = the whole peak segment, Decline = from end of peak segment to the next boundary
boom_start <- ti[cuts[seg_id]   + 1]
boom_end <- ti[cuts[seg_id+1]]
decline_start <- boom_end
decline_end <- ti[cuts[pmin(seg_id + 2, length(cuts))]]

# Create dummies
ps_df <- ps_df |> 
  mutate(
    boom_dummy = as.integer(date >= boom_start & date <= boom_end),
    decline_dummy = as.integer(date > decline_start & date <= decline_end)
  )

# Fitted piecewise trend for visual check
fit_tr <- fitted(bp_tr, breaks = length(idx))

# Plot 
ggplot(ps_df, aes(date, PS)) +
  geom_line(colour = "black") +
  geom_line(aes(y = as.numeric(fit_tr)), colour = "red", linewidth = 0.5) +
  geom_vline(xintercept = ti[idx], linetype = 2, colour = "blue") +
  annotate("rect", xmin = boom_start, xmax = boom_end, ymin = -Inf, ymax = Inf,
           alpha = 0.12, fill = "forestgreen") +
  annotate("rect", xmin = decline_start, xmax = decline_end, ymin = -Inf, ymax = Inf,
           alpha = 0.12, fill = "orange") +
  labs(
    title = "Mining Time Series of Boom and Decline Windows",
    subtitle = paste0(
      "Boom: ", boom_start, " \u2192 ", boom_end,
      "   |   Decline: ", decline_start, " \u2192 ", decline_end
    ),
    x = "Date", y = "Mining factor (Perth-Sydney Spread)"
  ) +
  theme_minimal()
```

To model the series with the dummy variables, essentially we fit an ARIMA model to the residuals after accounting for the boom and decline periods. The final selected model was an ARIMA(2,0,1) with the boom and decline dummies included as regressors.

```{r add-ps-dummy}
boom_start <- yearmonth(boom_start)
boom_end <- yearmonth(boom_end)
decline_start <- yearmonth(decline_start)
decline_end <- yearmonth(decline_end)

# Add in dummies for mining boom and decline periods
train_PS <- train_PS |> 
  mutate(
    boom    = as.integer(date >= boom_start    & date <= boom_end),
    decline = as.integer(date >= decline_start & date <= decline_end)
  )
```

```{r fit-ps-model}
# Fit ARIMA with residuals after accounting for the dummies 
fit_PS <- train_PS |>
  model(
    arima201 = ARIMA(PS ~ boom + decline + pdq(2,0,1) + PDQ(0,0,0))
  ) |> 
  select(arima201)

report(fit_PS)
fit_PS |> 
  gg_tsresiduals()
```

## Forecast Performance 

For the forecasting stage, we use the best-performing models (ARIMA(2,1,1) with Fourier terms (K = 2) for U and ARIMA(2,0,1) for PS) identified earlier to project values 10 years ahead (120 months). We generate forecasts with prediction intervals at both 80% and 95%. These intervals show the range in which future values are likely to fall, giving us a measure of uncertainty, an 80% interval means there is an 8-in-10 chance that the true value will lie within that range.

For evaluation, we set aside one year of data (2023) as a test set. Although a typical split is about 20% of the dataset, our series only spans 25 years, so using 20% would cut off too much valuable information for forecasting. A one-year test period strikes a better balance.

The forecasts indicate that the national trend is expected to rise, with an 80% prediction interval width of 0.6845, reflecting higher uncertainty. In contrast, the mining trend (PS) shows a flatter path, with an 80% interval width of 0.4538. This narrower range reflects greater stability, largely because we excluded the one-off boom and decline period, which would otherwise make the series more volatile.

```{r prediction-interval}
# Forecast - 10 years
h <- 120
fit_U <- fit_U |> select(arima211)
last_idx <- max(train_PS$date)  
future_PS <- tibble(date = last_idx + (1:h), boom = 0L, decline = 0L) |> as_tsibble(index = date)

fc_U  <- fabletools::forecast(fit_U,  h = h, level = c(80, 95))
fc_PS <- forecast::forecast(fit_PS, new_data = future_PS , h = h, level = c(80, 95))

# Produce fan with for U 
fcbands_U <- fc_U |> 
  as_tibble() |> 
  mutate(
    lo80 = quantile(U, 0.10),
    hi80 = quantile(U, 0.90),
    lo95 = quantile(U, 0.025),
    hi95 = quantile(U, 0.975),
    width80 = hi80 - lo80,
    width95 = hi95 - lo95
  ) |> 
  select(.model, date, mean = `.mean`, lo80, hi80, lo95, hi95, width80, width95) 

last_row_U <- fcbands_U |> filter(date == max(date))

# Forecast plot with fan width at last obs 
autoplot(fc_U, factor_trends_ts) +
  geom_segment(data = last_row_U,
               aes(x = date, xend = date, y = lo80, yend = hi80, colour = .model),
               linewidth = 0.8) +
  geom_text(data = last_row_U,
            aes(x = date, y = hi80,
                label = sprintf("w80 = %.4f", width80),
                colour = .model),
            vjust = -0.6, size = 2.5)

# Produce fan width for PS 
fcbands_PS <- fc_PS  |> 
  as_tibble() |> 
  mutate(
    lo80 = quantile(PS, 0.10),
    hi80 = quantile(PS, 0.90),
    lo95 = quantile(PS, 0.025),
    hi95 = quantile(PS, 0.975),
    width80 = hi80 - lo80,
    width95 = hi95 - lo95
  ) |> 
  select(.model, date, mean = `.mean`, lo80, hi80, lo95, hi95, width80, width95) 

last_row_PS <- fcbands_PS |> filter(date == max(date))

autoplot(fc_PS, factor_trends_ts) +
  geom_segment(data = last_row_PS,
               aes(x = date, xend = date, y = lo80, yend = hi80, colour = .model),
               linewidth = 0.8) +
  geom_text(data = last_row_PS,
            aes(x = date, y = hi80,
                label = sprintf("w80 = %.4f", width80),
                colour = .model),
            vjust = -0.6, size = 2.5)
```

The test set results show that the chosen model for U performs consistently with very low error (RMSE = 0.0076, negligible relative to the scale of U). For PS, the model incorporating boom/decline dummies produces out-of-sample forecasts that are on average about 7% off, which is acceptable given the series’ inherently difficult-to-predict behaviour (mining cycle is more volatile than the national market). While the ACF1 values indicate some remaining autocorrelation in the residuals, overall both models demonstrate solid performance.

```{r test-set-performance}
acc_U <- accuracy(fc_U, test_U) |>
  select(.model, .type, RMSE, MAE, MAPE)

acc_PS <- accuracy(fc_PS, test_PS) |>
  select(.model, .type, RMSE, MAE, MAPE)

# Add labels
acc_U  <- acc_U  |> mutate(series = "U (national)")
acc_PS <- acc_PS |> mutate(series = "PS (mining)")

# Combine tables 
acc_all <- bind_rows(acc_U, acc_PS) |>
  relocate(series, .model, .type) |> 
  print()
```

## Factor Model Application on Major City and SA4 Regions 

### Goodness of Fit

We now turn to evaluating how well the factor model captures the variation in individual major city and SA4 indexes. R² measures the share of variation in each major city or SA4 housing index that our factor model explains. 

**For major cities and rest-of-state:** Most city and regions have an R² value above 0.98. The extremes highlight clear spatial patterns:

* Top cities and rest-of-state: Large east-coast capitals and their surrounding belts.

* Bottom cities and rest-of-state: Smaller or resource-dependent markets.

```{r model-r2-city}
reg_city <- reg_coefs |> filter(region_level == "major_city")

# Rank the major cities by r2 and print top and bottom 10 
ranked_city <- reg_city |>
  arrange(desc(r2)) |>
  mutate(rank = row_number())

top10_city <- ranked_city |>
  slice_head(n = 5) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

bottom10_city <- ranked_city |>
  slice_tail(n = 5) |>
  arrange(r2) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

tbl_rank_city <- top10_city |>
  full_join(bottom10_city, by = "row", suffix = c("_Top", "_Bottom")) |>
  select(Rank_Top, Region_Top, R2_Top, Rank_Bottom, Region_Bottom, R2_Bottom)

kable(
  tbl_rank_city,
  caption = "Top 5 and Bottom 5 Major City and Area by R²",
  digits = 4,
  col.names = c("Rank", "Region", "R\u00B2", "Rank", "Region", "R\u00B2"),
  escape = FALSE
) |>
  kable_styling(full_width = TRUE,
                bootstrap_options = c("striped", "condensed")) |>
  add_header_above(c("Top 5" = 3, "Bottom 5" = 3)) |>
  column_spec(1, width = "4em") |>
  column_spec(4, width = "4em") |>
  column_spec(3, width = "6em") |>
  column_spec(6, width = "6em")
```

**For SA4 regions**: The SA4 regions shares the same stories. R² is tightly concentrated near 1.0 with the majority of SA4s exceed 0.98, with only a small left tail (a few around 0.92 – 0.95). The top and bottom lists identify the extremes and show recurring patterns across those regions: 

* Top 10 regions: Concentrated in the large east-coast capitals and surrounding belts.
* Bottom 10 regions : Dominated by Western Australia and Queensland’s mining exposed regions.

```{r model-r2-sa4}
# Filter the SA4 regions in the coefficient data 
reg_sa4 <- reg_coefs |> filter(region_level == "sa4_name")

# Rank the regions by r2 and print top and bottom 10 
ranked_sa4 <- reg_sa4 |>
  arrange(desc(r2)) |>
  mutate(rank = row_number())

top10_sa4 <- ranked_sa4 |>
  slice_head(n = 10) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

bottom10_sa4 <- ranked_sa4 |>
  slice_tail(n = 10) |>
  arrange(r2) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

tbl_rank_sa4 <- top10_sa4 |>
  full_join(bottom10_sa4, by = "row", suffix = c("_Top", "_Bottom")) |>
  select(Rank_Top, Region_Top, R2_Top, Rank_Bottom, Region_Bottom, R2_Bottom)

kable(
  tbl_rank_sa4,
  caption = "Top 10 and Bottom 10 SA4 Regions by R²",
  digits = 4,
  col.names = c("Rank", "Region", "R\u00B2", "Rank", "Region", "R\u00B2"),
  escape = FALSE
) |>
  kable_styling(full_width = TRUE,
                bootstrap_options = c("striped", "condensed")) |>
  add_header_above(c("Top 10" = 3, "Bottom 10" = 3)) |>
  column_spec(1, width = "4em") |>
  column_spec(4, width = "4em") |>
  column_spec(3, width = "6em") |>
  column_spec(6, width = "6em")
```

Both SA4 regions and major cities display highly concentrated R² distributions with major cities showing less variation.

```{r model-r2-comp}
dist_comp <- bind_rows(
  reg_sa4 |> transmute(group = "SA4 (n=88)", r2),
  reg_city |> transmute(group = "Major cities (n=15)", r2)
)
ggplot(dist_comp, aes(x = r2, y = group)) +
  ggdist::stat_slab(aes(thickness = after_stat(pdf)),
                    adjust = 1,
                    alpha = 0.5) +
  ggdist::stat_pointinterval(.width = c(0.5, 0.8, 0.95), point_size = 1.8) +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(title = "R² distribution: SA4 vs major cities", x = "R²", y = NULL) +
  theme_minimal(base_size = 12)
```
The choropleth reinforces the findings from the summary table, as we can see that the east-coast capitals and coastal belts from Melbourne through Sydney into the South-East of Queensland show the strongest fit,  with most regional NSW/VIC also high. While fit weakens inland and to the west, notably across Western Australia and along Queensland’s mining belts; The dark purple areas mark the lowest R² and are typically remote and resource-exposed regions with possible boom–bust timing and sparse transactions that our three factors don’t fully capture.

Hence, we’ll prioritise Western Australia and Queensland’s mining-exposed regions for residual diagnostics, where we expect autocorrelation and regime shifts for trial enhancements. 

```{r model-r2-dist-plot}
# Match names between reg_sa4 and sa4 shapefile
norm <- function(x) {
  x |>
    str_to_upper() |>
    str_replace_all("&", "AND") |>
    str_replace_all("[[:punct:]]", " ") |>
    str_squish()
}

# Filter city and geo coord 
major_cities <- c(
  "AUSTRALIAN CAPITAL TERRITORY",
  "GREATER ADELAIDE",
  "GREATER BRISBANE",
  "GREATER DARWIN",
  "GREATER HOBART",
  "GREATER MELBOURNE",
  "GREATER PERTH",
  "GREATER SYDNEY"
)

city_r2 <- reg_coefs |>
  filter(region %in% major_cities) |>
  transmute(city = region, r2)

city_coords <- tibble::tribble(
  ~city, ~lon, ~lat,
  "GREATER SYDNEY", 151.21, -33.87,
  "GREATER MELBOURNE", 144.96, -37.81,
  "GREATER BRISBANE", 153.03, -27.47,
  "GREATER PERTH", 115.86, -31.95,
  "GREATER ADELAIDE", 138.60, -34.93,
  "GREATER HOBART", 147.33, -42.88,
  "GREATER DARWIN", 130.84, -12.46,
  "AUSTRALIAN CAPITAL TERRITORY", 149.13, -35.28
)

city_mapdata <- city_r2 |> left_join(city_coords, by = "city")

# Prepare the 88 regions 
keep88 <- reg_sa4 |>
  mutate(region_norm = norm(region)) |>
  distinct(region_norm)

# Clean shapefile + keep regions + join r2 
sa4_r2 <- sa4 |>
  mutate(region_norm = norm(SA4_NAME21)) |>
  # drop empty geometries and special APS areas
  filter(!st_is_empty(geometry)) |>
  filter(!str_detect(region_norm, "^MIGRATORY\\s+OFFSHORE\\s+SHIPPING"),
         !str_detect(region_norm, "^NO\\s+USUAL\\s+ADDRESS")) |>
 semi_join(keep88, by = "region_norm") |> # Keep only the 88 sa4
  left_join( # Join r2 
    reg_sa4 |>
      mutate(region_norm = norm(region)) |>
      select(region_norm, r2),
    by = "region_norm"
  )

# Plot the r2 distribution 
ggplot() +
  geom_sf(data = sa4_r2, aes(fill = r2), linewidth = 0.1, colour = "grey70") +
  scale_fill_viridis_c(name = "SA4 R²", labels = percent) +
  geom_point(data = city_mapdata, aes(x = lon, y = lat), colour = "red", size = 1) +
  ggrepel::geom_label_repel(
  data = city_mapdata,
  aes(x = lon, y = lat, label = paste0(city, "\n", scales::percent(r2, 0.01))),
  size = 1.5, label.padding = unit(0.05, "lines"), label.r = unit(0.05, "lines"),    
  label.size = 0.15, box.padding = 0.2, seed = 42) +
  coord_sf() +
  labs(title = "Factor model R² distribution across cities & regions") +
  theme_void(base_size = 12)
```

### Residual Autocorrelation

At this stage, the model does not incorporate any time dynamics in the residuals. While it explains a substantial share of the variance for both cities and regions, the trending and autocorrelated nature of housing indexes means that the residuals themselves display extremely strong positive autocorrelation. This is evident in the Durbin–Watson statistics, which are close to zero across all regions: Major cities cluster around 0.02 to 0.05, while SA4 regions have a slightly broader range, extending up to 0.2, but still remain far below the benchmark value of 2. This pattern underscores the limitations of using a static regression framework without accounting for time-series dynamics.

```{r dw-stat}
reg_coefs |>
  group_by(region_level) |> 
  summarise(
    dw_min   = min(durbin_watson),
    dw_max   = max(durbin_watson),
    dw_mean  = mean(durbin_watson),
    dw_median= median(durbin_watson), 
    range = max(durbin_watson) - min(durbin_watson)
  )

ggplot(reg_coefs, aes(x = region_level, y = durbin_watson, fill = region_level)) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_boxplot(width = 0.1, outlier.shape = NA) +
  labs(title = "Durbin–Watson by region type",
       x = "", y = "Durbin–Watson") +
  coord_cartesian(ylim = c(0, 0.25)) + 
  theme_linedraw()
```

## Coefficient Pattern

While R² and Durbin–Watson assess overall fit and residual behaviour, they do not show how regions load onto each factor. Examining coefficient patterns adds this context, revealing whether markets move uniformly with the national trend or diverge due to mining influences.

The distribution of market coefficients is tightly centred around 1.0 with relatively little variation. This indicates that most regions respond in a similar way to the national housing trend. In contrast, the mining coefficients are far more dispersed with a long right tail and several extreme outliers, suggesting that exposure to mining-related dynamics differs substantially across regions.

The scatterplot analysis reveals a clear negative relationship between the market and mining coefficients. Regions with a strong loading on the national market factor tend to exhibit weaker or even negative loadings on the mining factor and vice versa.

```{r coef-pattern}
reg_coef_long <- reg_coefs |>
  select(region_level, market, mining) |>
  pivot_longer(c(market, mining),
               names_to = "factor", values_to = "coef")

# Density histogram 
ggplot(reg_coef_long, aes(x = coef, fill = factor)) +
  geom_histogram(aes(y = after_stat(density)),
                 bins = 30, , colour = "white") +
  geom_density(size = 0.5, fill = NA) +
  facet_wrap(~factor, scales = "free_x") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "Coefficient", y = "Density",
       title = "Coefficient distributions")

# Pairwise
cols <- c("market","mining")
ggpairs(
  reg_coefs,
  columns = match(cols, names(reg_coefs)),
  mapping = aes(colour = region_level),
  upper = list(continuous = wrap("cor", size = 3)),
  lower = list(continuous = wrap("smooth_loess", alpha = 0.6, size = 0.6)),
  diag  = list(continuous = wrap("densityDiag", alpha = 0.6))
) 
```



# Improve Model by Time-dependent Coefficients 


# Boundedness Test on the Factor Trends (WIP)

(WIP - examine numbers since we switched to PS; Add more methodologies with the forecast model should be built at this stage)

To justify including δPS alongside the national trend U in the factor model, we need to show that δPS is more bounded over time, meaning it stays within a narrower range and has lower long-term volatility by comparing δPS and U using overall statistics and rolling-window measures.

* The global summary (entire time period) of δPS and U covered several different measures, each of them captures different aspect of spread. The statistics includes: 
  * Standard deviation: The average distance of each observation to the series mean. If δPS has a smaller standard deviation than U, it indicates that δPS is more stable and less volatile over the entire period.
  * Interquartile range: The spread of the medium 50% of the data, if δPS has a tighter interquartile range than U, then it is generally more stable. 
  * Median aPSolute deviation: The median of abolute deviations from the mean. This measure is more robust to outliers. If the value of δPS is lower than U, it indicates that δPS has less extreme fluctuations.
  * Total range: The difference between the maximum and minimum values in the series, which examines the extreme swings in the series.  

We next applied rolling windows of 12 and 24 months to the same statistics. Rather than using the entire time span at once, this approach slides a fixed-length window along the series, calculating the variability measures at each step using only the most recent 12 or 24 months of data. This method highlights how the volatility and dispersion of δPS and U evolve over time. 

We use 12-month windows to smooth short-term fluctuations and capture typical annual housing cycles, helping assess if volatility is bounded within a year. The 24-month windows extend the view, showing whether this boundedness holds beyond yearly patterns and reflecting medium-term events.

To keep the report concise, we highlight two plots: the 12-month rolling standard deviation for short-term volatility and the 24-month rolling range for longer-term fluctuations.

Finally, we calculated the proportion of time (by 12 and 24 months respectively) in which δPS had a lower spread than U, giving a simple measure of how often it was more bounded.

```{r eval = FALSE}
# Load df 
factor_trends_ana <- factor_trends |>
  select(month_date, U = market, PS = mining) 

# Global comparison - full period 
global_summary <- factor_trends_ana |>
  summarise(
    sd_U = sd(U),
    sd_PS = sd(PS),
    var_U = var(U),
    var_PS = var(PS),
    iqr_U = IQR(U),
    iqr_PS = IQR(PS),
    range_U = diff(range(U)),
    range_PS = diff(range(PS)),
    mad_U = mad(U, center = median(U)),
    mad_PS = mad(PS, center = median(PS))
  ) 

global_ratio <- global_summary |>
  mutate(
    sd_ratio_PS_over_U = sd_PS / sd_U,
    iqr_ratio_PS_over_U = iqr_PS / iqr_U,
    mad_ratio_PS_over_U = mad_PS / mad_U,
    rng_ratio_PS_over_U = range_PS / range_U
  ) |>
  select(sd_ratio_PS_over_U,
         iqr_ratio_PS_over_U,
         mad_ratio_PS_over_U,
         rng_ratio_PS_over_U) 

kable(global_ratio, caption = "Global Summary Statistics") 
```

The global ratios are all well below 1, meaning δPS has consistently lower spread than U across all four measures. This supports δPS being more bounded overall.

```{r eval = FALSE}
# Rolling window config (12 and 24 months) 
w12 <- 12
w24 <- 24

# Functions for rolling statistics (SD, IQR, Range)
roll_sd   <- function(x, k)
  slide_dbl(x, sd, .before = k - 1, .complete = TRUE)

roll_iqr  <- function(x, k)
  slide_dbl(x, IQR, .before = k - 1, .complete = TRUE)

roll_rng  <- function(x, k)
  slide_dbl(x, function(vec)
    diff(range(vec)), .before = k - 1, .complete = TRUE)

# Compute rolling statistics
roll_stats <- factor_trends_ana |>
  mutate(
    # 12-month window
    sd12_U = roll_sd(U, w12),
    iqr12_U = roll_iqr(U, w12),
    rng12_U = roll_rng(U, w12),
    sd12_PS = roll_sd(PS, w12),
    iqr12_PS = roll_iqr(PS, w12),
    rng12_PS = roll_rng(PS, w12),
    # 24-month window
    sd24_U = roll_sd(U, w24),
    iqr24_U = roll_iqr(U, w24),
    rng24_U = roll_rng(U, w24),
    sd24_PS = roll_sd(PS, w24),
    iqr24_PS = roll_iqr(PS, w24),
    rng24_PS = roll_rng(PS, w24)
  )
```

```{r eval = FALSE}
# Rolling 12-month SD plot 
sd12_long <- roll_stats |>
  select(month_date, sd12_U, sd12_PS) |>
  pivot_longer(-month_date, names_to = "series", values_to = "sd12") 

ggplot(sd12_long, aes(month_date, sd12, color = series)) +
  geom_line() +
  labs(
    title = "Rolling 12-month Standard Deviation (SD)",
    subtitle = "Lower values show less volatility over a 12-month period",
    x = "Year",
    y = "SD"
  ) +
  theme_minimal()

# Rolling 24-month Range plot 
rng24_long <- roll_stats |>
  select(month_date, rng24_U, rng24_PS) |>
  pivot_longer(-month_date, names_to = "series", values_to = "rng24") 

ggplot(rng24_long, aes(month_date, rng24, color = series)) +
  geom_line() +
  labs(
    title = "Rolling 24-month Range",
    subtitle = "Wider range means bigger swings over the period",
    x = "Year", y = "Range"
  ) +
  theme_minimal()

# Summary table 
porp_summary <- tibble(
  period = c("12-month", "24-month"),
  prop_lower_sd = c(
    mean(roll_stats$sd12_PS < roll_stats$sd12_U, na.rm = TRUE),
    mean(roll_stats$sd24_PS < roll_stats$sd24_U, na.rm = TRUE)
  ),
  prop_lower_iqr = c(
    mean(roll_stats$iqr12_PS < roll_stats$iqr12_U, na.rm = TRUE),
    mean(roll_stats$iqr24_PS < roll_stats$iqr24_U, na.rm = TRUE)
  ),
  prop_lower_rng = c(
    mean(roll_stats$rng12_PS < roll_stats$rng12_U, na.rm = TRUE),
    mean(roll_stats$rng24_PS < roll_stats$rng24_U, na.rm = TRUE)
  )
)

kable(porp_summary, digits = 3, caption = "Proportion of Time δPS < U in Rolling Windows")
```

While the rolling-window plots and statistics give a more detailed picture: 

* The 12-month rolling standard deviation plot shows that δPS is often less volatile than U, but not consistently (occasional spikes narrow the gap).

* The 24-month rolling range plot shows δPS has wider swings at times, but also periods where it is more stable than U, highlighting its tendency to avoid large swings over longer periods.

* δPS has a lower spread than U in around 38.6% of the time for 12-month windows and 41.0% for 24-month windows, indicating it is more bounded (however less than half the time) in rolling windows.
