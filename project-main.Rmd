---
title: "Co-movement and Growth in Housing Markets: from PCA to cointegration and factor models"
author: "Yiran Yao"
date: "2025-08-04"
output:
  html_document:
    css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      fig.align = "center",
                      fig.width = 8, 
                      fig.height = 4
                      )
```

```{r library}
library(tidyverse)
library(janitor)
library(lubridate)
library(forecast)
library(fabletools)
library(fpp3)
library(urca)
library(broom)
library(tseries)
library(scales)
library(slider)
library(kableExtra)
library(patchwork)
library(sf)
library(distributional) 
#library(plotly)
```

```{r load-clean-data}
city_indexes <- read_csv("data/city_indexes.csv")
sa4_indexes <- read_csv("data/indexes_city_and_sa4.csv")
factor_trends <- read_csv("data/df_factor_trends.csv")
reg_coefs <- read_csv("data/df_reg_coefs.csv")

pcs <- read_csv("data/df_pcs.csv")
eofs_city <- read_csv("data/df_eofs_city.csv")
eofs <- read_csv("data/df_eofs.csv")

sa4 <- st_read("data/SA4_2021_AUST_SHP_GDA2020/SA4_2021_AUST_GDA2020.shp", quiet = TRUE)

city_indexes <- city_indexes |> 
  clean_names() 
```

# Introduction 

Traditional approaches to analysing housing indexes, such as hedonic or repeat-sales models, can introduce selection bias or violate underlying assumptions. Moreover, they fail to capture key regional, social, or macroeconomic factors that play a significant role in driving housing price movements.

In the recent research Sijp et al. (2025) propose a new approach that applies Principal Component Analysis (PCA) to city-level housing indexes, enabling the extraction of key drivers of price movements and offering a deeper understanding of the forces shaping housing markets.

The PCA results show that the first three principal components capture much of the behaviour of local price indexes, as a linear combination of their time series explains most of the variance in the housing index. Nevertheless, the PCA-based linear model lacks robustness across different time windows, and its coefficient estimates must be derived directly from the PCA procedure.

Our exploratory research extends the PCA approach by addressing these limitations through a factor linear model, which uses time series data directly, with independent variables serving as proxies for the PCA-derived principal components.


# Research Structure 

The national trend (U) exhibits a close correspondence with PC1 (market), while the Brisbane–Sydney spread (δBS) aligns with PC2 (mining). Given that these principal components account for the majority of variance in the index, our preliminary analysis focuses on U and δBS, with subsequent extensions planned.

The research will focus on the following directions: 

* Examine the characteristics of U and δBS. 

* Evaluate whether the existing U-adjusted δBS should be employed in place of the “Brisbane on Sydney” spread derived from the bris ∼ syd regression for modelling purposes.

* Whether the δBS serves as a more stable measure than U in the factor model.

* Whether the coefficient δ provides a valid replacement for PC2.

* How well the factor model captures the variation in individual SA4 indexes.

* How the model can be extended to a time-dependent version using moving windows with evolving coefficients.


## Exploratory Analysis 

The exploratory analysis utilises visualisations to characterise the dynamics of the national trend (U) and the Brisbane–Sydney spread (δBS), thereby clarifying their underlying properties.

```{r}
factor_trend_ts <- factor_trends |>
  transmute(
    month_date = yearmonth(as.Date(month_date)),
    U = as.numeric(market),
    BS = as.numeric(mining)
  ) |>
  as_tsibble(index = month_date)

# Time-series autoplot
p1 <- factor_trend_ts |> autoplot(U) + labs(title = "U: National Trend", x = "Month", y = "Index")
p2 <- factor_trend_ts |> autoplot(BS) + labs(title = "δBS: Brisbane–Sydney Spread", x = "Month", y = "Index")
p1 + p2
```

**Time plots:** 

* U rises steadily from 2003 to 2012, transitions into a slower growth phase until 2019, then undergoes a sharp regime shift during 2020–2022 (the COVID period) followed by persistently elevated and volatile levels, with growth rates remaining time-varying throughout.

* δBS displays a prolonged, uneven rise from 2003 to 2012, followed by a sharp regime shift, then COVID-related fluctuations, with the level adjusting and showing time-varying patterns.

```{r}
# Seasonal plots
p3  <- factor_trend_ts |> gg_season(U) + labs(title = "Seasonal Plot: U", x = "Month", y = "Index")
p4 <- factor_trend_ts |> gg_season(BS) + labs(title = "Seasonal Plot: δBS", x = "Month", y = "Index")
p3 + p4
```

**Seasonal plots:**

* U has apparent within-year slopes being dominated by the strong upward trend; no systematic month-of-year effects.

* δBS shows little evidence of month-to-month seasonality, with no particular month standing out, though its overall level varies between years.

```{r}
# Seasonal subseries
p5 <- factor_trend_ts |> gg_subseries(U) + labs(title = "Seasonal Subseries: U", x = "Month", y = "Index")
p6 <- factor_trend_ts |> gg_subseries(BS) + labs(title = "Seasonal Subseries: δBS", x = "Month", y = "Index")
p5 + p6
```

**Seasonal sub-series plot:** 

* Each monthly panel of U shows a steady upward rise with mean value across months remaining similar, reinforcing that the series is driven more by trend than by seasonality.

* The δBS seasonal sub-series plot reinforces the lack of a dominant seasonal pattern, as the monthly profiles are similar in shape and the corresponding means remain relatively flat and close to zero.

```{r}
# ACF
p7  <- factor_trend_ts |> ACF(U) |> autoplot() + labs(title = "ACF: U", x = "Lag", y = "ACF")
p8 <- factor_trend_ts |> ACF(BS) |> autoplot() + labs(title = "ACF: δBS", x = "Lag", y = "ACF")
p7 + p8
```

**ACF plot:** 

* Both series has very high autocorrelation with slow decay across many lags, indicating strong dependence on many prior months, though the variation of δBS is much more bounded than U. 


## Cointegration Test on Price Index between Brisbane and Sydney 

We use the Engle–Granger procedure to test whether the **raw Brisbane and Sydney price index** (from `city_index` data set) levels are cointegrated, that is - whether each series is non-stationary but some linear combination is stationary, implying a stable long-run relationship. 

The purpose of this test is to decide whether to regress the raw Brisbane index (μ Brisbane) on the raw Sydney index (μ Sydney) to obtain the coefficient, or to continue using the current δBS, which is derived by regressing the Sydney and Brisbane price indexes on the national trend U first.

```{r}
# Build monthly series of levels
city <- city_indexes |>
  transmute(
    month = yearmonth(floor_date(as.Date(month_date), "month")),
    bris  = as.numeric(greater_brisbane),
    syd   = as.numeric(greater_sydney)
  ) |>
  as_tsibble(index = month) 

# Unit-root checks
adf_bris_lvl  <- ur.df(city$bris, type = "trend", selectlags = "AIC")
adf_syd_lvl   <- ur.df(city$syd,  type = "trend", selectlags = "AIC")
adf_bris_diff <- ur.df(diff(city$bris), type = "none", selectlags = "AIC")
adf_syd_diff  <- ur.df(diff(city$syd),  type = "none", selectlags = "AIC")

# Long-run regression
cn_lm <- lm(bris ~ syd, data = as.data.frame(city))  
alpha <- coef(cn_lm)[2]
ect   <- resid(cn_lm)  

# ADF on residuals 
adf_ect <- ur.df(ect, type = "none", selectlags = "AIC")
summary(adf_ect)
```

The test statistic is –1.098, so we fail to reject the unit-root null. This result indicates no cointegration between the raw Brisbane and Sydney levels and the residuals are non-stationary, which means a single long-run equilibrium is not supported.

We therefore model a factor model and treat the current δBS as an informative second factor (a proxy for PC2), rather than assuming a tight long-run equilibrium between Brisbane and Sydney levels.

## Sydney-Brisbane Spread vs National Trend

To justify including δBS alongside the national trend U in the factor model, we need to show that δBS is more bounded over time, meaning it stays within a narrower range and has lower long-term volatility.

We approach this assumption from 3 angles:

* Method One uses past volatility and spread measures to compare past fluctuations in δBS and U. It is straightforward, does not rely on a model, and gives a clear historical picture.

* Method Two fits ARIMA models to both series and compares the width of 10-year forecast intervals. This gives a forward-looking view using well-known time series methods.

* Method Three runs simulation-based forecasts to estimate the possible future range of each series. This can capture uncertainty beyond ARIMA’s limits and show patterns like non-linear or extreme swings.

Combined, these methods integrate descriptive analysis, statistical modelling, and stochastic simulation to rigorously assess whether δBS is more bounded than U.

### Statistical Comparison 

In the first part, we compared δBS and U using overall statistics and rolling-window measures.

* The global summary (entire time period) of δBS and U covered several different measures, each of them captures different aspect of spread. The statistics includes: 
  * Standard deviation: The average distance of each observation to the series mean. If δBS has a smaller standard deviation than U, it indicates that δBS is more stable and less volatile over the entire period.
  * Interquartile range: The spread of the medium 50% of the data, if δBS has a tighter interquartile range than U, then it is generally more stable. 
  * Median absolute deviation: The median of absolute deviations from the mean. This measure is more robust to outliers. If the value of δBS is lower than U, it indicates that δBS has less extreme fluctuations.
  * Total range: The difference between the maximum and minimum values in the series, which examines the extreme swings in the series.  

We next applied rolling windows of 12 and 24 months to the same statistics. Rather than using the entire time span at once, this approach slides a fixed-length window along the series, calculating the variability measures at each step using only the most recent 12 or 24 months of data. This method highlights how the volatility and dispersion of δBS and U evolve over time. 

We use 12-month windows to smooth short-term fluctuations and capture typical annual housing cycles, helping assess if volatility is bounded within a year. The 24-month windows extend the view, showing whether this boundedness holds beyond yearly patterns and reflecting medium-term events.

To keep the report concise, we highlight two plots: the 12-month rolling standard deviation for short-term volatility and the 24-month rolling range for longer-term fluctuations.

Finally, we calculated the proportion of time (by 12 and 24 months respectively) in which δBS had a lower spread than U, giving a simple measure of how often it was more bounded.

```{r}
# Load df 
factor_trends_ana <- factor_trends |>
  select(month_date, U = market, BS = mining) 

# Global comparison - full period 
global_summary <- factor_trends_ana |>
  summarise(
    sd_U = sd(U),
    sd_BS = sd(BS),
    var_U = var(U),
    var_BS = var(BS),
    iqr_U = IQR(U),
    iqr_BS = IQR(BS),
    range_U = diff(range(U)),
    range_BS = diff(range(BS)),
    mad_U = mad(U, center = median(U)),
    mad_BS = mad(BS, center = median(BS))
  ) 

global_ratio <- global_summary |>
  mutate(
    sd_ratio_BS_over_U = sd_BS / sd_U,
    iqr_ratio_BS_over_U = iqr_BS / iqr_U,
    mad_ratio_BS_over_U = mad_BS / mad_U,
    rng_ratio_BS_over_U = range_BS / range_U
  ) |>
  select(sd_ratio_BS_over_U,
         iqr_ratio_BS_over_U,
         mad_ratio_BS_over_U,
         rng_ratio_BS_over_U) 

kable(global_ratio, caption = "Global Summary Statistics") 

# Summary tables 
#print(global_summary) 
#print(global_ratio)
```

The global ratios are all well below 1, meaning δBS has consistently lower spread than U across all four measures. This supports δBS being more bounded overall.

```{r}
# Rolling window config (12 and 24 months) 
w12 <- 12
w24 <- 24

# Functions for rolling statistics (SD, IQR, Range)
roll_sd   <- function(x, k)
  slide_dbl(x, sd, .before = k - 1, .complete = TRUE)

roll_iqr  <- function(x, k)
  slide_dbl(x, IQR, .before = k - 1, .complete = TRUE)

roll_rng  <- function(x, k)
  slide_dbl(x, function(vec)
    diff(range(vec)), .before = k - 1, .complete = TRUE)

# Compute rolling statistics
roll_stats <- factor_trends_ana |>
  mutate(
    # 12-month window
    sd12_U = roll_sd(U, w12),
    iqr12_U = roll_iqr(U, w12),
    rng12_U = roll_rng(U, w12),
    sd12_BS = roll_sd(BS, w12),
    iqr12_BS = roll_iqr(BS, w12),
    rng12_BS = roll_rng(BS, w12),
    # 24-month window
    sd24_U = roll_sd(U, w24),
    iqr24_U = roll_iqr(U, w24),
    rng24_U = roll_rng(U, w24),
    sd24_BS = roll_sd(BS, w24),
    iqr24_BS = roll_iqr(BS, w24),
    rng24_BS = roll_rng(BS, w24)
  )
```

```{r}
# Rolling 12-month SD plot 
sd12_long <- roll_stats |>
  select(month_date, sd12_U, sd12_BS) |>
  pivot_longer(-month_date, names_to = "series", values_to = "sd12") 

ggplot(sd12_long, aes(month_date, sd12, color = series)) +
  geom_line() +
  labs(
    title = "Rolling 12-month Standard Deviation (SD)",
    subtitle = "Lower values show less volatility over a 12-month period",
    x = "Year",
    y = "SD"
  ) +
  theme_minimal()

# Rolling 24-month Range plot 
rng24_long <- roll_stats |>
  select(month_date, rng24_U, rng24_BS) |>
  pivot_longer(-month_date, names_to = "series", values_to = "rng24") 

ggplot(rng24_long, aes(month_date, rng24, color = series)) +
  geom_line() +
  labs(
    title = "Rolling 24-month Range",
    subtitle = "Wider range means bigger swings over the period",
    x = "Year", y = "Range"
  ) +
  theme_minimal()

# Summary table 
porp_summary <- tibble(
  period = c("12-month", "24-month"),
  prop_lower_sd = c(
    mean(roll_stats$sd12_BS < roll_stats$sd12_U, na.rm = TRUE),
    mean(roll_stats$sd24_BS < roll_stats$sd24_U, na.rm = TRUE)
  ),
  prop_lower_iqr = c(
    mean(roll_stats$iqr12_BS < roll_stats$iqr12_U, na.rm = TRUE),
    mean(roll_stats$iqr24_BS < roll_stats$iqr24_U, na.rm = TRUE)
  ),
  prop_lower_rng = c(
    mean(roll_stats$rng12_BS < roll_stats$rng12_U, na.rm = TRUE),
    mean(roll_stats$rng24_BS < roll_stats$rng24_U, na.rm = TRUE)
  )
)

kable(porp_summary, digits = 3, caption = "Proportion of Time δBS < U in Rolling Windows")
```

While the rolling-window plots and statistics give a more detailed picture: 

* The 12-month rolling standard deviation plot shows that δBS is often less volatile than U, but not consistently (occasional spikes narrow the gap).

* The 24-month rolling range plot shows δBS has wider swings at times, but also periods where it is more stable than U, highlighting its tendency to avoid large swings over longer periods.

* δBS has a lower spread than U in around 38.6% of the time for 12-month windows and 41.0% for 24-month windows, indicating it is more bounded (however less than half the time) in rolling windows.

### ARIMA Forecasting Comparison

Following the descriptive analysis, in this section, we take a model-based approach to assess boundedness. 

We applied auto ARIMA to both δBS and U time series to produce 10-year ahead forecasts, with the sole purpose of comparing their long-term prediction interval widths. 

Using auto selection ensures each series is modelled appropriately without over-focusing on manual parameter tuning, keeping the emphasis on interval comparison as a measure of future boundedness.

```{r}
# Transform df to tsibble 
factor_trends_ts <- factor_trends |>
  transmute(date = yearmonth(month_date),
            U  = market,
            BS = mining) |> 
  as_tsibble(index = date)

# U is non stationary, I(1)
adf_U <- ur.df(factor_trends_ts$U, type="trend", lags=12)
kpss_U <- kpss.test(factor_trends_ts$U, null="Trend")
# BS also I(1)
adf_BS <- ur.df(factor_trends_ts$BS, type="drift", lags=12) 
kpss_BS <- kpss.test(factor_trends_ts$BS, null="Level")
# BS is stationary with a single structural break at position 53: "2002 Oct"
za <- ur.za(factor_trends_ts$BS, model = "both") 
```

U: big, smooth upward trend; seasonality is tiny (order 0.003–0.004 vs level ~1.5+); remainder has multi-year waves. 
BS: oscillates around ~0; no permanent trend; seasonality tiny; long multi-year swings; we also found a structural break (ZA test).
```{r}
# STL decompo plots 
U_stl <- factor_trends_ts |> 
  model(stl = STL(U))

U_stl |> 
  components() |> 
  autoplot() +
  labs(title = "STL Decomposition of National Trend U")

ggtsdisplay(factor_trends_ts$U, main = "ACF and PACF with National Trend U")

BS_stl <- factor_trends_ts |> 
  model(stl = STL(BS))

BS_stl |> 
  components() |> 
  autoplot() +
  labs(title = "STL Decomposition of Mining Trend BS")

ggtsdisplay(factor_trends_ts$BS, main = "ACF and PACF with Mining Trend BS")
```

```{r}
# Split testing and training 
train_U <- factor_trends_ts |> 
  select(date, U) |>
  filter(date < yearmonth("2023 Jan")) 

test_U <- factor_trends_ts |> 
  select(date, U) |>
  filter(date >= yearmonth("2023 Jan")) 

train_BS <- factor_trends_ts |> 
  select(date, BS) |>
  filter(date < yearmonth("2023 Jan"))

test_BS <- factor_trends_ts |> 
  select(date, BS) |>
  filter(date >= yearmonth("2023 Jan"))
```


```{r}
# Add knots 
add_pw <- function(df){
  df %>%
    mutate(
      D_2008 = as.integer(date >= yearmonth("2008-01")), # GFC/credit conditions shift
      D_2012 = as.integer(date >= yearmonth("2012-01")), # end of the mining-investment boom / regime change
      R_2020 = pmax(0, (year(date) - 2020) * 12 + (month(date) - 3)) #covid 
    )
}
train_U <- add_pw(train_U)
test_U  <- add_pw(test_U)

# Fitting U - ETS need to transform the data 
fit_U <- train_U %>%
  model(
    rw_drift  = ARIMA(U ~ 1 + pdq(0,1,0)),                 # RW + drift - benchmark 
  #  arima011  = ARIMA(U ~ 1 + pdq(0,1,1)),                 # (0,1,1) with drift
    arima211  = ARIMA(U ~ 1 + pdq(2,1,1) + fourier(K = 2)),                # (2,1,1) with drift + 6-month harmonic
  #  ets_damp  = ETS(  U ~ error("A") + trend("Ad") + season("N")),  # ETS damped trend
  #  piecewise = ARIMA(U ~ 1 + pdq(0,1,0) + D_2008 + D_2012 + R_2020) # piecewise
  #  ets_auto = ETS(U)
  )

# Examine fit  
glance(fit_U) |> arrange(AICc)

fit_U |> 
  select(arima211) |>
  gg_tsresiduals()

# Ljung-Box test
lb_U <- bind_rows(
  augment(fit_U |> select(rw_drift))  |> features(.innov, ljung_box, lag=12, dof=0) |> mutate(.model="rw_drift"),
 # augment(fit_U |> select(arima011)) |> features(.innov, ljung_box, lag=12, dof=1) |> mutate(.model="arima011"),
  augment(fit_U |> select(arima211)) |> features(.innov, ljung_box, lag=12, dof=1) |> mutate(.model="arima111"),
 # augment(fit_U |> select(piecewise))|> features(.innov, ljung_box, lag=12, dof=0) |> mutate(.model="piecewise"),
  #augment(fit_U |> select(ets_damp)) |> features(.innov, ljung_box, lag=12, dof=0) |> mutate(.model="ets_damp")
) |> print()

fc_U <- fit_U |> 
  select(arima211, rw_drift) |> 
  forecast(test_U, level = c(80,95))

autoplot(fc_U, test_U) + scale_fill_brewer(palette = "Blues") + labs(title = "Forecasts on Test Set for U")
```

The auto ARIMA model selected ARIMA(0,2,0)(2,0,0)[12] for the national trend U and ARIMA(1,1,2) for the Brisbane–Sydney spread δBS. We have also examined the residuals of both fitted models, which appear to be white noise, indicating that the models adequately capture the underlying dynamics of both series.

Since applying two levels of differencing implies a quadratic trend, it may indicate over-differencing for an economic series that exhibits drift. Our visual inspection already suggests the presence of drift, and an Augmented Dickey–Fuller test (run separately) further confirms this with a Phi statistic significant at the 5% level. To avoid clutter, we do not include the test output in this report.

Given this, we manually fitted an ARIMA(0,1,0) model with a constant (random walk with drift) in parallel for U and compared its AICc value against the auto-selected ARIMA model. The auto ARIMA model is objectively better by AICc and diagnostics, so we will report the auto ARIMA for completeness but also include an ARIMA(1,0,1) with a constant as a benchmark for comparison since it has a stronger intuition. 

```{r}
# Forecast - 10 years
h <- 120
fc_U  <- fabletools::forecast(fit_U,  h = h, level = c(80, 95))
# fc_BS <- fabletools::forecast(fit_BS, h = h, level = c(80, 95))

# Fan width 
  fcbands_U <- fc_U |> 
  as_tibble() |> 
  mutate(
    lo80 = quantile(U, 0.10),
    hi80 = quantile(U, 0.90),
    lo95 = quantile(U, 0.025),
    hi95 = quantile(U, 0.975),
    width80 = hi80 - lo80,
    width95 = hi95 - lo95
  ) |> 
  select(.model, date, mean = `.mean`, lo80, hi80, lo95, hi95, width80, width95) 
  
width_summary <- fcbands_U |>
  group_by(.model) |> 
  summarise(
    width80_range = max(width80) - min(width80),
    width95_range = max(width95) - min(width95)
  ) |> print()

last_row <- fcbands_U |> filter(date == max(date))

# Forecast plot with fan width at last obs 
autoplot(fc_U, factor_trends_ts) +
  geom_segment(data = last_row,
               aes(x = date, xend = date, y = lo80, yend = hi80, colour = .model),
               linewidth = 0.8) +
  geom_text(data = last_row,
            aes(x = date, y = hi80,
                label = sprintf("w80 = %.4f", width80),
                colour = .model),
            vjust = -0.6, size = 2.5)

# Plot the forecasts
#autoplot(fc_U, factor_trends_ts)
# autoplot(fc_BS, factor_trends_ts) 
```

From the forecast fan plot, we can first see that the random walk with drift produces very narrow forecast intervals, because it assumes only linear variance growth with horizon and ignores short-run autocorrelation. By contrast, the auto-ARIMA specification captures AR and MA dynamics, which propagate uncertainty further into the future, resulting in wider intervals. 

The two alternative models for the national trend U lead to very different conclusions when compared with δBS. The random walk with drift produces unrealistically narrow intervals, while the auto-ARIMA specification not only achieves a much lower AICc but also yields forecast intervals that better reflect the underlying uncertainty. For this reason, we continue with the auto-ARIMA model in our boundedness comparison.

Based on the average 10-year forecast ranges, the δBS spread consistently shows narrower prediction intervals than the national trend U. This indicates that the spread is likely to remain more contained over the next decade, a conclusion supported by both the summary statistics and the forecast visualisations.

```{r eval = FALSE}
set.seed(123)

# Simulation 
n <- 5000 
sim_U  <- fit_U |> 
  select(auto_U) |> 
  generate(h = h, times = n, bootstrap = TRUE)

sim_BS <- fit_BS %>%
  generate(h = h, times = n, bootstrap = TRUE)
```

```{r eval = FALSE}
bands_U <- sim_U %>%
  as_tibble() %>%
  group_by(date) %>%
  summarise(
    p50  = median(.sim, na.rm = TRUE),
    lo80 = quantile(.sim, 0.10, na.rm = TRUE),
    hi80 = quantile(.sim, 0.90, na.rm = TRUE),
    lo95 = quantile(.sim, 0.025, na.rm = TRUE),
    hi95 = quantile(.sim, 0.975, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(width80 = hi80 - lo80, width95 = hi95 - lo95) %>%
  mutate(series = "U")

bands_BS <- sim_BS %>%
  as_tibble() %>%
  group_by(date) %>%
  summarise(
    p50  = median(.sim, na.rm = TRUE),
    lo80 = quantile(.sim, 0.10, na.rm = TRUE),
    hi80 = quantile(.sim, 0.90, na.rm = TRUE),
    lo95 = quantile(.sim, 0.025, na.rm = TRUE),
    hi95 = quantile(.sim, 0.975, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(width80 = hi80 - lo80, width95 = hi95 - lo95) %>%
  mutate(series = "δBS")

# End of horizon fan widths
end_widths <- bind_rows(bands_U, bands_BS) |> 
  slice_max(order_by = date, n = 1, by = series) |> 
  select(series, width80, width95)

kable(end_widths, caption = "End of Horizon Fan Widths for δBS and U")
```

The fan width at the end of the forecast horizon indicates that the uncertainty of U is approximately nine times greater than that of δBS.

```{r eval = FALSE}
# Stay in historical 90% band prob 
lo_U  <- quantile(factor_trends_ts$U,  0.05, na.rm = TRUE)
hi_U  <- quantile(factor_trends_ts$U,  0.95, na.rm = TRUE)
lo_BS <- quantile(factor_trends_ts$BS, 0.05, na.rm = TRUE)
hi_BS <- quantile(factor_trends_ts$BS, 0.95, na.rm = TRUE)

pr_U <- sim_U |>
  as_tibble() |>
  group_by(.rep) |>
  summarise(
    path_min = min(.sim, na.rm = TRUE),
    path_max = max(.sim, na.rm = TRUE),
    .groups = "drop"
  ) |>
  summarise(stay_90 = mean(path_min >= lo_U & path_max <= hi_U)) %>%
  pull(stay_90)

pr_BS <- sim_BS |>
  as_tibble() |>
  group_by(.rep) |>
  summarise(
    path_min = min(.sim, na.rm = TRUE),
    path_max = max(.sim, na.rm = TRUE),
    .groups = "drop"
  ) |>
  summarise(stay_90 = mean(path_min >= lo_BS &
                             path_max <= hi_BS)) %>%
  pull(stay_90)

stay_prob_tbl <- tibble(
  series = c("U", "δBS"),
  stay_90 = c(pr_U, pr_BS)
)

kable(stay_prob_tbl, caption = "Probability of Staying in Historical 90% Band")
```

δBS exhibits a 26.0% probability of remaining within its historical 90% confidence band, whereas U demonstrates a 0% probability. 

```{r eval = FALSE}
# MAD over horizon 
last_U  <- factor_trends_ts |> slice_max(date, n = 1) |> pull(U)
last_BS <- factor_trends_ts |> slice_max(date, n = 1) |> pull(BS)

devs_U <- sim_U |>
  group_by(.rep) |>
  summarise(max_dev = max(abs(.sim - last_U), na.rm = TRUE), .groups = "drop")

devs_BS <- sim_BS |>
  group_by(.rep) |>
  summarise(max_dev = max(abs(.sim - last_BS), na.rm = TRUE), .groups = "drop")

md_U_median <- median(devs_U$max_dev, na.rm = TRUE)
md_U_p90    <- quantile(devs_U$max_dev, 0.90, na.rm = TRUE)

md_BS_median <- median(devs_BS$max_dev, na.rm = TRUE)
md_BS_p90    <- quantile(devs_BS$max_dev, 0.90, na.rm = TRUE)

max_dev_tbl <- tibble::tibble(
  series         = c("U", "δBS"),
  median_max_dev = c(md_U_median, md_BS_median),
  p90_max_dev    = c(md_U_p90,    md_BS_p90)
)

kable(max_dev_tbl, caption = "Maximum Deviation Statistics for δBS and U")
```

Over the forecast horizon, the mean absolute deviation indicates that δBS exhibits both a lower median deviation and a lower 90th-percentile maximum deviation compared to U.

## Evaluating δBS as a Proxy for PC2

Following the earlier evaluation of cointegration and boundedness, we next assess whether the constructed δBS can serve as a valid proxy for PC2 in the factor model. This assessment is carried out by visually comparing the shape of the two standardised series and statistically testing their correlation.

The two series display very similar shapes, with a high correlation (>96%). The scatterplot illustrates why they are not identical: the points form loops rather than a perfect diagonal, reflecting small lead–lag effects as housing cycles unfold. This arises because PC2 is a statistical construct from PCA, while δBS is a regression-based Sydney–Brisbane spread. Nevertheless, given the very high correlation, δBS can be considered a valid proxy for PC2 in the factor model, with the minor differences not being statistically significant.”

```{r}
# Extract variables BS and PC2 
BS <- factor_trends |>
  select(month_date, mining) |>
  rename("period" = "month_date")

Z2 <- pcs |> 
  select(mode, pc_value, period) |> 
  filter(mode == 1) |>
  select(-mode)

# Join and scale with mean 0 variance 1
compare_BS_Z2 <- BS |>
  inner_join(Z2, by = "period") |>
  mutate(
    mining_std = as.numeric(scale(mining, center = TRUE, scale = TRUE)), 
    z2_std     = as.numeric(scale(pc_value, center = TRUE, scale = TRUE))
  )

# Plot the series 
ggplot(compare_BS_Z2, aes(x = as.Date(period))) +
  geom_line(aes(y = mining_std, colour = "δBS (Mining factor)")) +
  geom_line(aes(y = z2_std, colour = "PC2 (z2)")) +
  labs(x = "Period", y = "Index Value", colour = "Series") +
  theme_minimal()

# Calculate and plot the correlation  
BS_Z2_corr <- cor(compare_BS_Z2$mining, compare_BS_Z2$pc_value, use = "complete.obs")
cat("Correlation between δBS (mining) and PC2 (Z2):", BS_Z2_corr, "\n")

ggplot(compare_BS_Z2, aes(x = pc_value, y = mining)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm",
              se = FALSE,
              colour = "red") +
  labs(title = "Correlation between δBS (Mining factor) and PC2", x = "PC2 (Z2)", y = "δBS (Mining)") +
  theme_minimal()
```

## Baseline Factor Model Fit on SA4 regions

### Identify under-performing regions

We now turn to evaluating how well the factor model captures the variation in individual SA4 indexes.

R² measures the share of variation in each SA4 housing index that our factor model explains. Across SA4 regions, the mean R² is approximately 0.988 and the median is 0.993, indicating the model captures almost all of the observed variation. 


```{r}
# Filter the SA4 regions in the coefficient data 
reg_sa4 <- reg_coefs |> filter(region_level == "sa4_name")

ggplot(reg_sa4, aes(x = r2)) +
  geom_histogram(bins = 20) +
  stat_bin(
    bins = 20,
    geom = "text",
    aes(y = after_stat(count), label = after_stat(percent(
      count / sum(count), accuracy = 1 # Add % for counts at each bar 
    ))),
    vjust = -0.4,
    size = 3
  ) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.12))) +
  labs(title = "Distribution of R² across SA4 regions", x = "R²", y = "Count")

# Rank the regions by r2 and print top and bottom 15 
ranked <- reg_sa4 |>
  arrange(desc(r2)) |>
  mutate(rank = row_number())

top10 <- ranked |>
  slice_head(n = 10) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

bottom10 <- ranked |>
  slice_tail(n = 10) |>
  arrange(r2) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

tbl_rank <- top10 |>
  full_join(bottom10, by = "row", suffix = c("_Top", "_Bottom")) |>
  select(Rank_Top, Region_Top, R2_Top, Rank_Bottom, Region_Bottom, R2_Bottom)

kable(
  tbl_rank,
  caption = "Top 10 and Bottom 10 SA4 Regions by R²",
  digits = 4,
  col.names = c("Rank", "Region", "R\u00B2", "Rank", "Region", "R\u00B2"),
  escape = FALSE
) |>
  kable_styling(full_width = TRUE,
                bootstrap_options = c("striped", "condensed")) |>
  add_header_above(c("Top 10" = 3, "Bottom 10" = 3)) |>
  column_spec(1, width = "4em") |>
  column_spec(4, width = "4em") |>
  column_spec(3, width = "6em") |>
  column_spec(6, width = "6em")
```

R² is tightly concentrated near 1.0 with the majority of SA4s exceed 0.98, with only a small left tail (a few around 0.92 – 0.95). The top and bottom lists identify the extremes and show recurring patterns across those regions: 

* Top 10 regions: Concentrated in the large east-coast capitals and surrounding belts.
* Bottom 10 regions : Dominated by Western Australia and Queensland’s mining exposed regions.

```{r}
# Match names between reg_sa4 and sa4 shapefile
norm <- function(x) {
  x |>
    str_to_upper() |>
    str_replace_all("&", "AND") |>
    str_replace_all("[[:punct:]]", " ") |>
    str_squish()
}

# Prepare the 88 regions to keep
keep88 <- reg_sa4 |>
  mutate(region_norm = norm(region)) |>
  distinct(region_norm)

# Clean shapefile + keep only the 88 + join r2 
sa4_r2 <- sa4 |>
  mutate(region_norm = norm(SA4_NAME21)) |>
  # drop empty geometries and special ABS areas
  filter(!st_is_empty(geometry)) |>
  filter(!str_detect(region_norm, "^MIGRATORY\\s+OFFSHORE\\s+SHIPPING"),
         !str_detect(region_norm, "^NO\\s+USUAL\\s+ADDRESS")) |>
  semi_join(keep88, by = "region_norm") |> # Keep only the 88 sa4
  left_join( # Join r2 
    reg_sa4 |>
      mutate(region_norm = norm(region)) |>
      select(region_norm, r2),
    by = "region_norm"
  )

# Plot the r2 distribution 
ggplot(sa4_r2) +
  geom_sf(aes(fill = r2), linewidth = 0.1) +
  coord_sf() +
  scale_fill_viridis_c(name = "R²") +
  labs(title = "SA4 R² from factor model") +
  theme_void()
```

The choropleth reinforces the findings from the summary table, as we can see that the east-coast capitals and coastal belts from Melbourne through Sydney into the South-East of Queensland show the strongest fit,  with most regional NSW/VIC also high. While fit weakens inland and to the west, notably across Western Australia and along Queensland’s mining belts; The dark purple areas mark the lowest R² and are typically remote and resource-exposed regions with possible boom–bust timing and sparse transactions that our three factors don’t fully capture.

Hence, we’ll prioritise Western Australia and Queensland’s mining-exposed regions for residual diagnostics, where we expect autocorrelation and regime shifts for trial enhancements. 

### Inspect residual 

```{r}
#pivot your SA4 index (wide → long)
options(scipen = 999, digits = 10)

sa4_indexes_long <- sa4_indexes |>
  mutate(month_date = as.Date(month_date)) |>
  pivot_longer(-month_date, names_to = "region", values_to = "y") 

# Rename coefficients to avoid duplicates 
coefs <- reg_sa4 |>
  rename(beta_market = market,
         beta_mining = mining,
         beta_lifestyle = lifestyle)

# compute fitted and residual for every r and t
panel <- sa4_indexes_long |>
  left_join(
    factor_trends |>
      mutate(month_date = as.Date(month_date)) |>
      select(month_date, market, mining, lifestyle),
    by = "month_date"
  ) |>
  left_join(coefs, by = "region") |>
  mutate(
    fitted = alpha + beta_market*market + beta_mining*mining + beta_lifestyle*lifestyle,
    resid  = y - fitted
  )
```

# Forecast with Factor Model 
