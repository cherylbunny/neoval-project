---
title: "Housing Market Dynamics"
subtitle: "From PCA to Factor Models"
author: "Yiran Yao"
date: "2025-08-04"
output:
  html_document:
    css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      fig.align = "center",
                      fig.width = 8, 
                      fig.height = 4
                      )
```

```{r library}
library(tidyverse)
library(janitor)
library(lubridate)
library(forecast)
library(fabletools)
library(fpp3)
library(urca)
library(broom)
library(tseries)
library(scales)
library(slider)
library(kableExtra)
library(patchwork)
library(sf)
library(distributional) 
library(glue)
library(readxl)
#library(plotly)
```

```{r load-clean-data}
# Raw housing index 
city_indexes <- read_csv("data/city_indexes.csv")
sa4_indexes <- read_csv("data/indexes_city_and_sa4.csv")

# PC proxy with identified characteristic
factor_trends <- read_csv("data/df_factor_trends.csv")

# Static regression coef 
reg_coefs <- read_csv("data/df_reg_coefs.csv")

# PC score and loadings 
pcs <- read_csv("data/df_pcs.csv")
eofs_city <- read_csv("data/df_eofs_city.csv")
eofs <- read_csv("data/df_eofs.csv")

# Economic data - need cleaning 
#gdp_mining <- read_xlsx("data/data.xlsx")
#exchange_rate <- read_xlsx("data/key_economic_data.xlsx")
#mining_expenditure <- read_xlsx("data/mining_expenditure.xlsx", sheet = 2)

# For plotting SA4 geo 
sa4 <- st_read("data/SA4_2021_AUST_SHP_GDA2020/SA4_2021_AUST_GDA2020.shp", quiet = TRUE)

# Cleaning city names 
city_indexes <- city_indexes |> 
  clean_names() 
```

```{r compute-mining-ps}
# Compute the Perth Sydney Spread 
PS_raw <- city_indexes |> 
  select(month_date, greater_sydney, greater_perth)

U_raw <- factor_trends |> 
  select(month_date, market) 

com_PS <- PS_raw |> inner_join(U_raw, by = "month_date")

# Extract the numeric values and compute alpha 
mu_perth   <- com_PS$`greater_perth`
mu_sydney  <- com_PS$`greater_sydney`
U          <- com_PS$market   

alpha <- cov(mu_perth, U, use = "complete.obs") /
         cov(mu_sydney, U, use = "complete.obs")

# Add in PS spread to factor_trend as mining_ps 
com_PS <- com_PS |> 
  mutate(mining_ps = `greater_perth` - alpha * `greater_sydney`) |> 
  select(month_date, mining_ps)

factor_trends <- factor_trends |> 
  inner_join(com_PS, by = "month_date")
```

```{r find-pc3-proxy, eval = FALSE}
# Prepare the data we need - city pc loadings, pc scores, lifestyle trend 
pc3_pxy <- eofs_city |> 
  filter(mode == 2) |> 
  arrange(loading_value)

Z3 <- pcs |> 
  filter(mode == 2) |>  
  select(period, pc_value)

# Melbourne – Rest of QLD
MQ_raw <- city_indexes |> 
  select(month_date, rest_of_nsw, greater_melbourne)

U_raw <- factor_trends |> 
  select(month_date, market) 

com_MQ <- MQ_raw |> inner_join(U_raw, by = "month_date")

mu_rest_nsw   <- com_MQ$`rest_of_nsw`
mu_melbourne  <- com_MQ$`greater_melbourne`
U          <- com_MQ$market 

alpha1 <- cov(mu_rest_nsw, U, use = "complete.obs") /
         cov(mu_melbourne, U, use = "complete.obs")

com_MQ <- com_MQ |> 
  mutate(lifestyle_1 = `rest_of_nsw` - alpha1 * `greater_melbourne`) |> 
  select(month_date, lifestyle_1)

factor_trends_1 <- factor_trends |> 
  inner_join(com_MQ, by = "month_date")

lifestyle_cmp <- factor_trends_1 |> 
  select(month_date, lifestyle, lifestyle_1) |>
  rename(period = month_date) |> 
  inner_join(Z3, by = "period") |> 
  mutate(
    z3_std     = as.numeric(scale(pc_value)),
    lifestyleorg_std = as.numeric(scale(lifestyle)),
    lifestyle1_std = as.numeric(scale(lifestyle_1))
  )

plot_lifestyle_cmp <- lifestyle_cmp |> 
  select(period, z3_std, lifestyleorg_std, lifestyle1_std ) |> 
  pivot_longer(-period, names_to = "series", values_to = "value") |> 
  mutate(series = recode(series,
                         z3_std = "PC3",
                         lifestyleorg_std = "δlifestyle",
                         lifestyle1_std = "nsw_mel")) |> 
  ggplot(aes(x = as.Date(period), y = value, colour = series)) +
  geom_line() +
  labs(x = "Period", y = "Standardised value", colour = "Series",
       title = "Current ") +
  theme_minimal()

plot_lifestyle_cmp 
```

# Introduction 

Traditional approaches to analysing housing indexes, such as hedonic or repeat-sales models, can introduce selection bias or violate underlying assumptions. Moreover, they fail to capture key regional, social, or macroeconomic factors that play a significant role in driving housing price movements.

In the recent research Sijp et al. (2025) propose a new approach that applies Principal Component Analysis (PCA) to regional SA4 level housing indexes, enabling the extraction of key drivers of price movements and offering a deeper understanding of the forces shaping housing markets.

The PCA results show that the first three principal components capture much of the behaviour of local price indexes, as a linear combination of their time series explains most of the variance in the housing index. Nevertheless, the PCA-based linear model lacks robustness across different time windows, and its coefficient estimates must be derived directly from the PCA procedure.

Our exploratory research extends the PCA approach by addressing these limitations through a factor linear model, which uses time series data directly, with independent variables serving as proxies for the PCA-derived principal components.

# Moving from PCA to Factor Model (WIP)

The national trend (U) exhibits a close correspondence with PC1 (market), while the Perth–Sydney spread (δPS) aligns with PC2 (mining). Given that these principal components account for the majority of variance in the index, our preliminary analysis focuses on U and δPS, with subsequent extensions planned. (WIP - might add lifestyle)

-   Explain on more of why did we move on from PCA to factor model
-   Explain how did we find the factors (like how did we take national as PC1, PS as PC2...)
-   Might move add cointegration test to this section if needed as it explains how we formulate the alpha 

We examined whether PC2 is better represented by the Brisbane–Sydney or Perth–Sydney spread by comparing the shapes of the standardised series and testing their correlations with PC2. The Perth–Sydney spread aligns more closely with PC2: The scatterplot shows a distribution that falls more evenly along the regression line, and its correlation with PC2 is slightly higher (98% versus 96% for the Brisbane–Sydney spread). While the difference is minor and not statistically significant, the Perth–Sydney spread is the stronger candidate as a proxy for PC2.

```{r examine-pc-corr}
# Join PC2 with both series 
Z2 <- pcs |> 
  filter(mode == 1) |>  
  select(period, pc_value)

mining_cmp <- factor_trends |> 
  select(month_date, mining, mining_ps) |>
  rename(period = month_date) |> 
  inner_join(Z2, by = "period") |> 
  mutate(
    z2_std     = as.numeric(scale(pc_value)),
    deltaBS_std = as.numeric(scale(mining)),
    deltaPS_std = as.numeric(scale(mining_ps))
  )

# Plot PC2 with both series 
plot_ts <- mining_cmp |> 
  select(period, z2_std, deltaBS_std, deltaPS_std) |> 
  pivot_longer(-period, names_to = "series", values_to = "value") |> 
  mutate(series = recode(series,
                         z2_std = "PC2",
                         deltaBS_std = "δBS",
                         deltaPS_std = "δPS")) |> 
  ggplot(aes(x = as.Date(period), y = value, colour = series)) +
  geom_line() +
  labs(x = "Period", y = "Standardised value", colour = "Series",
       title = "PC2 vs δBS and δPS (standardised)") +
  theme_minimal()

# Compare correlation 
corr_bs <- cor(mining_cmp$pc_value, mining_cmp$mining,   use = "complete.obs")
corr_ps <- cor(mining_cmp$pc_value, mining_cmp$mining_ps, use = "complete.obs")

# Scatter plot 
scatter_df <- mining_cmp |> 
  select(pc_value, mining, mining_ps) |> 
  pivot_longer(cols = c(mining, mining_ps),
               names_to = "spread", values_to = "spread_value") |> 
  mutate(spread = recode(spread,
                         mining    = glue("δBS (r = {round(corr_bs, 3)})"),
                         mining_ps = glue("δPS (r = {round(corr_ps, 3)})")))

plot_scatter <- ggplot(scatter_df, aes(x = pc_value, y = spread_value)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ spread, scales = "free_y") +
  labs(title = "PC2 vs spreads", x = "PC2", y = "Spread value") +
  theme_minimal()

plot_ts
plot_scatter 
```

```{r cointegration-test, eval = FALSE}
# Build monthly series of levels
city <- city_indexes |>
  transmute(
    month = yearmonth(floor_date(as.Date(month_date), "month")),
    bris  = as.numeric(greater_brisbane),
    syd   = as.numeric(greater_sydney)
  ) |>
  as_tsibble(index = month) 

# Unit-root checks
adf_bris_lvl  <- ur.df(city$bris, type = "trend", selectlags = "AIC")
adf_syd_lvl   <- ur.df(city$syd,  type = "trend", selectlags = "AIC")
adf_bris_diff <- ur.df(diff(city$bris), type = "none", selectlags = "AIC")
adf_syd_diff  <- ur.df(diff(city$syd),  type = "none", selectlags = "AIC")

# Long-run regression
cn_lm <- lm(bris ~ syd, data = as.data.frame(city))  
alpha <- coef(cn_lm)[2]
ect   <- resid(cn_lm)  

# ADF on residuals 
adf_ect <- ur.df(ect, type = "none", selectlags = "AIC")
summary(adf_ect)
```

## Data Source and Limitaions (WIP)

# Exploratory Analysis 

The exploratory analysis utilises visualisations to characterise the dynamics of the national trend (U) and the Perth–Sydney spread (δPS), thereby clarifying their underlying properties.

```{r autoplot}
factor_trend_ts <- factor_trends |>
  transmute(
    month_date = yearmonth(as.Date(month_date)),
    U = as.numeric(market),
    PS = as.numeric(mining_ps)
  ) |>
  as_tsibble(index = month_date)

# Time-series autoplot
p1 <- factor_trend_ts |> autoplot(U) + labs(title = "U: National Trend", x = "Month", y = "Index")
p2 <- factor_trend_ts |> autoplot(PS) + labs(title = "δPS: Perth–Sydney Spread", x = "Month", y = "Index")

p1 + p2 
```

**Time-series plots:** 

* U rises steadily from 2003 to 2012, transitions into a slower growth phase until 2019, then undergoes a sharp regime shift during 2020–2022 (the COVID period) followed by persistently elevated and volatile levels, with growth rates remaining time-varying throughout.

* δPS displays a prolonged, uneven rise from 2003 to 2012, followed by a sharp regime shift, then COVID-related fluctuations, with the level adjusting and showing time-varying patterns.

```{r season-plot}
# Seasonal plots
p3  <- factor_trend_ts |> gg_season(U) + labs(title = "Seasonal Plot: U", x = "Month", y = "Index")
p4 <- factor_trend_ts |> gg_season(PS) + labs(title = "Seasonal Plot: δPS", x = "Month", y = "Index")
p3 + p4
```

**Seasonal plots:**

* U has apparent within-year slopes being dominated by the strong upward trend; no systematic month-of-year effects.

* δPS shows little evidence of month-to-month seasonality, with no particular month standing out, though its overall level varies between years.

```{r subseries-plot}
# Seasonal subseries
p5 <- factor_trend_ts |> gg_subseries(U) + labs(title = "Seasonal Subseries: U", x = "Month", y = "Index")
p6 <- factor_trend_ts |> gg_subseries(PS) + labs(title = "Seasonal Subseries: δPS", x = "Month", y = "Index")
p5 + p6
```

**Seasonal sub-series plot:** 

* Each monthly panel of U shows a steady upward rise with mean value across months remaining similar, reinforcing that the series is driven more by trend than by seasonality.

* The δPS seasonal sub-series plot reinforces the lack of a dominant seasonal pattern, as the monthly profiles are similar in shape and the corresponding means remain relatively flat and close to zero.


# Forecasting with the Factor Model 

With the factors now established, we proceed to fit time series forecasting models to each series individually. As a first step, we examine the series in more detail. STL decomposition plots allow us to closely inspect the trend, seasonality, and remainder components, which helps determine the most suitable forecasting models for the series’ characteristics.

Stationary tests (ADF and KPSS) indicate that both the national index (U) and the Perth–Sydney spread (PS) are non-stationary, so differencing is required. However, based on economic intuition that the PS series may be mean-reverting, we applied the Zivot–Andrews (ur.za) test. This confirmed that PS is in fact stationary, but with a single structural break occurring in October 2002.

The dataset was partitioned into training and testing subsets, with one year of observations allocated to the testing set. Given that the data are recorded at a monthly frequency and span only 25 years, this allocation represents an optimal balance between ensuring sufficient data for model training while preserving an adequate sample for evaluation.

```{r trans-ts}
# Transform df to tsibble 
factor_trends_ts <- factor_trends |>
  transmute(date = yearmonth(month_date),
            U  = market,
            PS = mining) |> 
  as_tsibble(index = date) 
```

```{r stationary-test, eval = FALSE}
# U is non stationary, I(1)
adf_U <- ur.df(factor_trends_ts$U, type="trend", lags=12)
kpss_U <- kpss.test(factor_trends_ts$U, null="Trend")
# PS also I(1)
adf_PS <- ur.df(factor_trends_ts$PS, type="drift", lags=12) 
kpss_PS <- kpss.test(factor_trends_ts$PS, null="Level")
# PS is stationary with a single structural break at position 53: "2002 Oct"
za <- ur.za(factor_trends_ts$PS, model = "both") 
```

## National Trend 

The STL decomposition of the national trend U reveals a smooth and persistent upward trajectory in the trend component, while the seasonal component is of relatively minor magnitude, however the annual seasonality is strong and repeating. To stabilise variation in the seasonal component, a Box–Cox transformation was applied using the Guerrero method, which produced a λ value of 0.9. Given its proximity to one, the transformation had a negligible effect on the modelling outcome. The remainder component displays mostly small noise, with some bigger shocks.

The modelling strategy was guided by exploratory data analysis and the STL decomposition, which suggested a random walk with drift, `ARIMA(U ∼ pdq(0,1,0))`, as an appropriate baseline specification. 

To refine the model, we subsequently evaluated neighbouring specifications with alternative autoregressive and moving average terms, using AICc values and residual diagnostics as selection criteria. 

In the modelling process, we evaluated a range of specifications informed both by diagnostic checks and prior knowledge. Residual analysis suggested the presence of semi-annual dynamics, which motivated the inclusion of Fourier terms with `K = 2`. We also investigated seasonal extensions with `P = 1`, alongside piecewise specifications with knots aligned to major economic events: the Global Financial Crisis (2008), the end of the mining investment boom (2012), and the onset of the COVID-19 pandemic (2020). These variations were assessed with respect to residual behaviour, statistical significance, and their ability to capture shocks apparent in the remainder component.

Among the fitted models, two candidates emerged as particularly strong. The first, an ARIMA(2,1,1) with Fourier terms (K = 2) and drift, achieved a Ljung–Box p-value above 5%, indicating residuals consistent with white noise. A second competitive specification combined Fourier terms (K = 2) with intervention dummies (step and pulse) and an ARIMA(2,1,1) structure. While both models satisfied the residual whiteness criterion, their information criteria diverged: the AICc values differed by approximately 20 to 40 points in favour of the intervention model. This highlights a central trade-off in model selection: balancing residual diagnostics against information criteria, and ultimately emphasising forecast performance as the guiding criterion for choosing between these specifications.

```{r fitting-u}
# Identify fits 
U_stl <- factor_trends_ts |> 
  model(stl = STL(U))

U_stl |> 
  components() |> 
  autoplot() +
  labs(title = "STL Decomposition of National Trend U")

ggtsdisplay(factor_trends_ts$U, main = "ACF and PACF with National Trend U")

# Split testing and training 
train_U <- factor_trends_ts |> 
  select(U, date) |> 
  filter(date < yearmonth("2023 Jan")) 

test_U <- factor_trends_ts |> 
  select(U, date) |> 
  filter(date >= yearmonth("2023 Jan")) 
```

```{r}
# Function to add knots - improvement can be done by examining the mining expenditure data 
add_pw <- function(df) {
  df |> 
    mutate(
      # steps
      D_2008 = as.integer(date >= yearmonth("2008 Jan")),
      D_2012 = as.integer(date >= yearmonth("2012 Jan")),
      R_2020 = pmax(0L, as.integer(yearmonth(date) - yearmonth("2020 Mar"))),
      step_2020_03  = as.integer(date >= yearmonth("2020 Mar")),
      pulse_2020_04 = as.integer(date ==  yearmonth("2020 Apr"))
    )
}

train_U <- add_pw(train_U)

# Fit models 
fit_U <- train_U |> 
  model(
    autoarima = ARIMA(U),
    rw_drift  = ARIMA(U ~ 1 + pdq(0,1,0)),
    arima211  = ARIMA(U ~ 1 + fourier(K = 2) + pdq(2,1,1)),
    arima110  = ARIMA(U ~ 1 + fourier(K = 2) + pdq(1,1,0)),
    arima111  = ARIMA(U ~ 1 + fourier(K = 2) + pdq(1,1,1)),
    sarima211 = ARIMA(U ~ 1 + pdq(2,1,1) + PDQ(1,0,0)),
    piece = ARIMA(U ~ 1 + pdq(1,1,1) + D_2008 + D_2012 + R_2020),
    step = ARIMA(U ~ fourier(K = 2) + step_2020_03 + pulse_2020_04 + pdq(2,1,1))
  )

# Examine fit  
glance(fit_U) |> arrange(AICc)

fit_U |> 
  select(step) |>
  gg_tsresiduals()

fit_U |> 
  select(arima211) |>
  gg_tsresiduals()

# Ljung-Box test
lb_tbl <- fit_U |>
  augment() |> 
  features(.innov, ljung_box, lag = 12) |>
  select(.model, lb_stat, lb_pvalue) |> 
  arrange(desc(lb_pvalue)) |> 
  print()

```


## Mining Trend 
PS: oscillates around ~0; no permanent trend; seasonality tiny; long multi-year swings; we also found a structural break (ZA test).
```{r}
PS_stl <- factor_trends_ts |> 
  model(stl = STL(PS))

PS_stl |> 
  components() |> 
  autoplot() +
  labs(title = "STL Decomposition of Mining Trend PS")

ggtsdisplay(factor_trends_ts$PS, main = "ACF and PACF with Mining Trend PS")

train_PS <- factor_trends_ts |> 
  select(date, PS) |>
  filter(date < yearmonth("2023 Jan"))

test_PS <- factor_trends_ts |> 
  select(date, PS) |>
  filter(date >= yearmonth("2023 Jan"))
```

## Forecast Performance 

```{r}
# Forecast - 10 years
h <- 120
fc_U  <- fabletools::forecast(fit_U,  h = h, level = c(80, 95))
# fc_PS <- fabletools::forecast(fit_PS, h = h, level = c(80, 95))

# Fan width 
  fcbands_U <- fc_U |> 
  as_tibble() |> 
  mutate(
    lo80 = quantile(U, 0.10),
    hi80 = quantile(U, 0.90),
    lo95 = quantile(U, 0.025),
    hi95 = quantile(U, 0.975),
    width80 = hi80 - lo80,
    width95 = hi95 - lo95
  ) |> 
  select(.model, date, mean = `.mean`, lo80, hi80, lo95, hi95, width80, width95) 
  
width_summary <- fcbands_U |>
  group_by(.model) |> 
  summarise(
    width80_range = max(width80) - min(width80),
    width95_range = max(width95) - min(width95)
  ) |> print()

last_row <- fcbands_U |> filter(date == max(date))

# Forecast plot with fan width at last obs 
autoplot(fc_U, factor_trends_ts) +
  geom_segment(data = last_row,
               aes(x = date, xend = date, y = lo80, yend = hi80, colour = .model),
               linewidth = 0.8) +
  geom_text(data = last_row,
            aes(x = date, y = hi80,
                label = sprintf("w80 = %.4f", width80),
                colour = .model),
            vjust = -0.6, size = 2.5)

# Plot the forecasts
#autoplot(fc_U, factor_trends_ts)
# autoplot(fc_PS, factor_trends_ts) 
```

From the forecast fan plot, we can first see that the random walk with drift produces very narrow forecast intervals, because it assumes only linear variance growth with horizon and ignores short-run autocorrelation. By contrast, the auto-ARIMA specification captures AR and MA dynamics, which propagate uncertainty further into the future, resulting in wider intervals. 

The two alternative models for the national trend U lead to very different conclusions when compared with δPS. The random walk with drift produces unrealistically narrow intervals, while the auto-ARIMA specification not only achieves a much lower AICc but also yields forecast intervals that better reflect the underlying uncertainty. For this reason, we continue with the auto-ARIMA model in our boundedness comparison.

Based on the average 10-year forecast ranges, the δPS spread consistently shows narrower prediction intervals than the national trend U. This indicates that the spread is likely to remain more contained over the next decade, a conclusion supported by both the summary statistics and the forecast visualisations.


## Factor Model Application on SA4 regions 

### Identify under-performing regions - Baseline Factor Model Fit on SA4 regions

We now turn to evaluating how well the factor model captures the variation in individual SA4 indexes.

R² measures the share of variation in each SA4 housing index that our factor model explains. Across SA4 regions, the mean R² is approximately 0.988 and the median is 0.993, indicating the model captures almost all of the observed variation. 


```{r}
# Filter the SA4 regions in the coefficient data 
reg_sa4 <- reg_coefs |> filter(region_level == "sa4_name")

ggplot(reg_sa4, aes(x = r2)) +
  geom_histogram(bins = 20) +
  stat_bin(
    bins = 20,
    geom = "text",
    aes(y = after_stat(count), label = after_stat(percent(
      count / sum(count), accuracy = 1 # Add % for counts at each bar 
    ))),
    vjust = -0.4,
    size = 3
  ) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.12))) +
  labs(title = "Distribution of R² across SA4 regions", x = "R²", y = "Count")

# Rank the regions by r2 and print top and bottom 15 
ranked <- reg_sa4 |>
  arrange(desc(r2)) |>
  mutate(rank = row_number())

top10 <- ranked |>
  slice_head(n = 10) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

bottom10 <- ranked |>
  slice_tail(n = 10) |>
  arrange(r2) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

tbl_rank <- top10 |>
  full_join(bottom10, by = "row", suffix = c("_Top", "_Bottom")) |>
  select(Rank_Top, Region_Top, R2_Top, Rank_Bottom, Region_Bottom, R2_Bottom)

kable(
  tbl_rank,
  caption = "Top 10 and Bottom 10 SA4 Regions by R²",
  digits = 4,
  col.names = c("Rank", "Region", "R\u00B2", "Rank", "Region", "R\u00B2"),
  escape = FALSE
) |>
  kable_styling(full_width = TRUE,
                bootstrap_options = c("striped", "condensed")) |>
  add_header_above(c("Top 10" = 3, "Bottom 10" = 3)) |>
  column_spec(1, width = "4em") |>
  column_spec(4, width = "4em") |>
  column_spec(3, width = "6em") |>
  column_spec(6, width = "6em")
```

R² is tightly concentrated near 1.0 with the majority of SA4s exceed 0.98, with only a small left tail (a few around 0.92 – 0.95). The top and bottom lists identify the extremes and show recurring patterns across those regions: 

* Top 10 regions: Concentrated in the large east-coast capitals and surrounding belts.
* Bottom 10 regions : Dominated by Western Australia and Queensland’s mining exposed regions.

```{r}
# Match names between reg_sa4 and sa4 shapefile
norm <- function(x) {
  x |>
    str_to_upper() |>
    str_replace_all("&", "AND") |>
    str_replace_all("[[:punct:]]", " ") |>
    str_squish()
}

# Prepare the 88 regions to keep
keep88 <- reg_sa4 |>
  mutate(region_norm = norm(region)) |>
  distinct(region_norm)

# Clean shapefile + keep only the 88 + join r2 
sa4_r2 <- sa4 |>
  mutate(region_norm = norm(SA4_NAME21)) |>
  # drop empty geometries and special APS areas
  filter(!st_is_empty(geometry)) |>
  filter(!str_detect(region_norm, "^MIGRATORY\\s+OFFSHORE\\s+SHIPPING"),
         !str_detect(region_norm, "^NO\\s+USUAL\\s+ADDRESS")) |>
  semi_join(keep88, by = "region_norm") |> # Keep only the 88 sa4
  left_join( # Join r2 
    reg_sa4 |>
      mutate(region_norm = norm(region)) |>
      select(region_norm, r2),
    by = "region_norm"
  )

# Plot the r2 distribution 
ggplot(sa4_r2) +
  geom_sf(aes(fill = r2), linewidth = 0.1) +
  coord_sf() +
  scale_fill_viridis_c(name = "R²") +
  labs(title = "SA4 R² from factor model") +
  theme_void()
```

The choropleth reinforces the findings from the summary table, as we can see that the east-coast capitals and coastal belts from Melbourne through Sydney into the South-East of Queensland show the strongest fit,  with most regional NSW/VIC also high. While fit weakens inland and to the west, notably across Western Australia and along Queensland’s mining belts; The dark purple areas mark the lowest R² and are typically remote and resource-exposed regions with possible boom–bust timing and sparse transactions that our three factors don’t fully capture.

Hence, we’ll prioritise Western Australia and Queensland’s mining-exposed regions for residual diagnostics, where we expect autocorrelation and regime shifts for trial enhancements. 

### Inspect residual 

```{r}
#pivot your SA4 index (wide → long)
options(scipen = 999, digits = 10)

sa4_indexes_long <- sa4_indexes |>
  mutate(month_date = as.Date(month_date)) |>
  pivot_longer(-month_date, names_to = "region", values_to = "y") 

# Rename coefficients to avoid duplicates 
coefs <- reg_sa4 |>
  rename(beta_market = market,
         beta_mining = mining,
         beta_lifestyle = lifestyle)

# compute fitted and residual for every r and t
panel <- sa4_indexes_long |>
  left_join(
    factor_trends |>
      mutate(month_date = as.Date(month_date)) |>
      select(month_date, market, mining, lifestyle),
    by = "month_date"
  ) |>
  left_join(coefs, by = "region") |>
  mutate(
    fitted = alpha + beta_market*market + beta_mining*mining + beta_lifestyle*lifestyle,
    resid  = y - fitted
  )
```

# Improve Model by Time-dependent Coefficients 


# Boundedness Test on the Factor Trends (WIP)

(WIP - examine numbers since we switched to PS; Add more methodologies with the forecast model should be built at this stage)

To justify including δPS alongside the national trend U in the factor model, we need to show that δPS is more bounded over time, meaning it stays within a narrower range and has lower long-term volatility by comparing δPS and U using overall statistics and rolling-window measures.

* The global summary (entire time period) of δPS and U covered several different measures, each of them captures different aspect of spread. The statistics includes: 
  * Standard deviation: The average distance of each observation to the series mean. If δPS has a smaller standard deviation than U, it indicates that δPS is more stable and less volatile over the entire period.
  * Interquartile range: The spread of the medium 50% of the data, if δPS has a tighter interquartile range than U, then it is generally more stable. 
  * Median aPSolute deviation: The median of abolute deviations from the mean. This measure is more robust to outliers. If the value of δPS is lower than U, it indicates that δPS has less extreme fluctuations.
  * Total range: The difference between the maximum and minimum values in the series, which examines the extreme swings in the series.  

We next applied rolling windows of 12 and 24 months to the same statistics. Rather than using the entire time span at once, this approach slides a fixed-length window along the series, calculating the variability measures at each step using only the most recent 12 or 24 months of data. This method highlights how the volatility and dispersion of δPS and U evolve over time. 

We use 12-month windows to smooth short-term fluctuations and capture typical annual housing cycles, helping assess if volatility is bounded within a year. The 24-month windows extend the view, showing whether this boundedness holds beyond yearly patterns and reflecting medium-term events.

To keep the report concise, we highlight two plots: the 12-month rolling standard deviation for short-term volatility and the 24-month rolling range for longer-term fluctuations.

Finally, we calculated the proportion of time (by 12 and 24 months respectively) in which δPS had a lower spread than U, giving a simple measure of how often it was more bounded.

```{r}
# Load df 
factor_trends_ana <- factor_trends |>
  select(month_date, U = market, PS = mining) 

# Global comparison - full period 
global_summary <- factor_trends_ana |>
  summarise(
    sd_U = sd(U),
    sd_PS = sd(PS),
    var_U = var(U),
    var_PS = var(PS),
    iqr_U = IQR(U),
    iqr_PS = IQR(PS),
    range_U = diff(range(U)),
    range_PS = diff(range(PS)),
    mad_U = mad(U, center = median(U)),
    mad_PS = mad(PS, center = median(PS))
  ) 

global_ratio <- global_summary |>
  mutate(
    sd_ratio_PS_over_U = sd_PS / sd_U,
    iqr_ratio_PS_over_U = iqr_PS / iqr_U,
    mad_ratio_PS_over_U = mad_PS / mad_U,
    rng_ratio_PS_over_U = range_PS / range_U
  ) |>
  select(sd_ratio_PS_over_U,
         iqr_ratio_PS_over_U,
         mad_ratio_PS_over_U,
         rng_ratio_PS_over_U) 

kable(global_ratio, caption = "Global Summary Statistics") 
```

The global ratios are all well below 1, meaning δPS has consistently lower spread than U across all four measures. This supports δPS being more bounded overall.

```{r}
# Rolling window config (12 and 24 months) 
w12 <- 12
w24 <- 24

# Functions for rolling statistics (SD, IQR, Range)
roll_sd   <- function(x, k)
  slide_dbl(x, sd, .before = k - 1, .complete = TRUE)

roll_iqr  <- function(x, k)
  slide_dbl(x, IQR, .before = k - 1, .complete = TRUE)

roll_rng  <- function(x, k)
  slide_dbl(x, function(vec)
    diff(range(vec)), .before = k - 1, .complete = TRUE)

# Compute rolling statistics
roll_stats <- factor_trends_ana |>
  mutate(
    # 12-month window
    sd12_U = roll_sd(U, w12),
    iqr12_U = roll_iqr(U, w12),
    rng12_U = roll_rng(U, w12),
    sd12_PS = roll_sd(PS, w12),
    iqr12_PS = roll_iqr(PS, w12),
    rng12_PS = roll_rng(PS, w12),
    # 24-month window
    sd24_U = roll_sd(U, w24),
    iqr24_U = roll_iqr(U, w24),
    rng24_U = roll_rng(U, w24),
    sd24_PS = roll_sd(PS, w24),
    iqr24_PS = roll_iqr(PS, w24),
    rng24_PS = roll_rng(PS, w24)
  )
```

```{r}
# Rolling 12-month SD plot 
sd12_long <- roll_stats |>
  select(month_date, sd12_U, sd12_PS) |>
  pivot_longer(-month_date, names_to = "series", values_to = "sd12") 

ggplot(sd12_long, aes(month_date, sd12, color = series)) +
  geom_line() +
  labs(
    title = "Rolling 12-month Standard Deviation (SD)",
    subtitle = "Lower values show less volatility over a 12-month period",
    x = "Year",
    y = "SD"
  ) +
  theme_minimal()

# Rolling 24-month Range plot 
rng24_long <- roll_stats |>
  select(month_date, rng24_U, rng24_PS) |>
  pivot_longer(-month_date, names_to = "series", values_to = "rng24") 

ggplot(rng24_long, aes(month_date, rng24, color = series)) +
  geom_line() +
  labs(
    title = "Rolling 24-month Range",
    subtitle = "Wider range means bigger swings over the period",
    x = "Year", y = "Range"
  ) +
  theme_minimal()

# Summary table 
porp_summary <- tibble(
  period = c("12-month", "24-month"),
  prop_lower_sd = c(
    mean(roll_stats$sd12_PS < roll_stats$sd12_U, na.rm = TRUE),
    mean(roll_stats$sd24_PS < roll_stats$sd24_U, na.rm = TRUE)
  ),
  prop_lower_iqr = c(
    mean(roll_stats$iqr12_PS < roll_stats$iqr12_U, na.rm = TRUE),
    mean(roll_stats$iqr24_PS < roll_stats$iqr24_U, na.rm = TRUE)
  ),
  prop_lower_rng = c(
    mean(roll_stats$rng12_PS < roll_stats$rng12_U, na.rm = TRUE),
    mean(roll_stats$rng24_PS < roll_stats$rng24_U, na.rm = TRUE)
  )
)

kable(porp_summary, digits = 3, caption = "Proportion of Time δPS < U in Rolling Windows")
```

While the rolling-window plots and statistics give a more detailed picture: 

* The 12-month rolling standard deviation plot shows that δPS is often less volatile than U, but not consistently (occasional spikes narrow the gap).

* The 24-month rolling range plot shows δPS has wider swings at times, but also periods where it is more stable than U, highlighting its tendency to avoid large swings over longer periods.

* δPS has a lower spread than U in around 38.6% of the time for 12-month windows and 41.0% for 24-month windows, indicating it is more bounded (however less than half the time) in rolling windows.
