---
title: "Regional Housing Market Dynamics in Australia"
subtitle: "A PCA-Based Approach to Factor Models"
author: "Yiran Yao"
date: "2025-08-04"
output:
  html_document:
    css: styles.css
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      fig.align = "center",
                      fig.width = 8, 
                      fig.height = 4
                      )
```

```{r library}
library(tidyverse)
library(janitor)
library(lubridate)
library(forecast)
library(fabletools)
library(fpp3)
library(urca)
library(broom)
library(tseries)
library(scales)
library(slider)
library(kableExtra)
library(patchwork)
library(sf)
library(distributional) 
library(glue)
library(GGally)
library(zoo)
library(strucchange)
library(slider)
library(glmnet)
library(lmtest)  
```

```{r load-clean-data}
# Raw housing index 
city_indexes <- read_csv("data/city_indexes.csv")
sa4_indexes <- read_csv("data/sa4_indexes.csv")

# PC proxy
factor_trends <- read_csv("data/df_factor_trends.csv") 
factor_trends <- factor_trends |> 
  select(-lifestyle_melb_nsw) |> 
  mutate(month_date = yearmonth(month_date)) |> 
  as_tsibble(index = month_date)

# Static regression coef - better do this regression yourself 
reg_coefs <- read_csv("data/df_reg_coefs.csv") 

# PC score and loadings
pcs <- read_csv("data/df_pcs.csv")
eofs_city <- read_csv("data/df_eofs_city.csv")
eofs_sa4 <- read_csv("data/df_eofs_sa4.csv")

# For plotting SA4 geographic map
sa4 <- st_read("data/SA4_2021_AUST_SHP_GDA2020/SA4_2021_AUST_GDA2020.shp", quiet = TRUE)

# Cleaning names 
city_indexes <- city_indexes |> 
  clean_names() 

# Remove duplicated ACT row
reg_coefs <- reg_coefs |>
  filter(!(region == "AUSTRALIAN CAPITAL TERRITORY" & duplicated(region)))
```

# Introduction 

Traditional approaches to analysing housing indexes, such as hedonic or repeat-sales models, can introduce selection bias or violate underlying assumptions. Moreover, they fail to capture key regional, social, or macroeconomic factors that play a significant role in driving housing price movements.

In the recent research Sijp et al. (2025) propose a new approach that applies Principal Component Analysis (PCA) to regional SA4 level housing indexes, enabling the extraction of key drivers of price movements and offering a deeper understanding of the forces shaping housing markets.

The PCA results show that the first three principal components capture much of the behavior of local price indexes, as a linear combination of their time series explains most of the variance in the housing index. Nevertheless, the PCA-based linear model lacks robustness across different time windows, and its coefficient estimates must be derived directly from the PCA procedure.

Our exploratory research extends the PCA approach by addressing these limitations through a factor linear model, which uses time series data directly, with independent variables serving as proxies for the PCA-derived principal components. We model the factor proxies as time series using forecasting techniques to obtain their projected values. The forecasted factor series are subsequently incorporated into the linear model to generate projected housing price indexes, with time-varying coefficients included to capture evolving dynamics over time.

The resulting factor model can be applied for a variety of purposes:
-   To understand the key drivers of housing price movements at a national and regional level.
-   To generate forecasts of housing price indexes based on projected factor values.
-   To assess the impact of macroeconomic variables on housing prices through the regression coefficients.

# Methodologies and Datasets 

## Methodlogies 

### PCA 

Before introducing the factor model, Principal Component Analysis (PCA) was first applied to the original housing indexes. It is therefore useful to briefly revisit PCA and how it allowed us to identify specific factors. This work was originally conducted by Will Sijp, the supervisor of this project, whose detailed analysis can be found in the references. What follows is a short recap.

PCA is a statistical method that reduces complex datasets into a smaller number of uncorrelated components, each capturing a dominant source of variation. In our project, the PC1 explains 96% of the variance, while PC2 and PC3 account for 2.5% and 0.7%, respectively. Together, these three components capture nearly all the variability in the housing indexes.

The loadings from the PCA highlight which regions contribute most strongly to each component. For example, PC2, labelled the “mining” factor, shows high positive loadings in resource-dependent regions, while service-oriented regions display negative loadings. This pattern suggests that PC2 captures the influence of the mining sector on housing markets. By contrast, PC1, the “market” factor, has positive loadings across nearly all regions, reflecting the common national trend in housing prices.

### Factor-Based Substitutes for Principal Components

After defining the factors associated with each principal component (PC), we introduce a linear regression that combines selected housing indexes to approximate the PCs, which are referred to as factor proxies. The objective is to identify interpretable city or regional combinations that closely mirror the PCA-derived components. A proxy is considered valid if it achieves a correlation of at least 96% with the corresponding PC, providing statistical evidence of its suitability. This procedure is not strictly systematic but rather heuristic, involving an iterative search for the most representative mix of cities or regions. More specifically:

* For PC1 proxy the national spread (U), the factor model is specified as: \(\mu_r = \beta_r U + \varepsilon\)

* For the PC2 proxy, the mining spread (Perth–Sydney spread, $\delta_{PS}$), the factor model is specified as: \(\mu_r = \beta_r U + \lambda_r \delta_{PS} + \varepsilon\)

* For PC3 proxy, the lifestyle spread (Melbourne-Rest of NSW spread, $\delta_{lifestyle}$), the factor model is specified as: \(\mu_r = \beta_r U + \lambda_r \delta_{PS} + \gamma_r \delta_{lifestyle} + \varepsilon\

### Forecasting of Factors

Once the factor proxies were defined, we modelled each factor series separately using time series forecasting methods. The process included exploratory analysis, stationarity checks, and STL decomposition to characterise the data. Guided by these diagnostics, we selected suitable ARIMA-type models, enhancing them with Fourier terms to capture seasonality and intervention dummies to address structural breaks.

### Integration into a Factor Modelling Framework

The linear model regresses each regional housing index on the factor proxies, yielding coefficients that represent the sensitivity of local markets to the underlying factors: \(\mu_r = \beta_{National} + \lambda_r \delta_{Mining} + \gamma_r \delta_{lifestyle} + \varepsilon\), the coefficients β, λ and γ are estimated for each region r. These coefficients help to capture the response of the local market to each factor. Model performance was evaluated using R-squared and residual diagnostics to check that key assumptions held. Forecasts of the factor proxies were then substituted into the model to obtain projected housing indexes. To account for changing dynamics, coefficients were re-estimated with rolling windows, allowing them to vary over time and better capture evolving market conditions.

### Factor Model Enhancement 

Residual diagnostics from the static factor model indicated autocorrelation, suggesting that regional price dynamics were not fully captured by contemporaneous factors alone. To address this, the model was extended using an ARIMAX framework, retaining the same factor proxies as exogenous variables while incorporating autoregressive and moving-average terms to account for temporal dependence. An expanding-window approach was adopted to re-estimate models as new data became available, enabling coefficients to adapt to evolving market conditions and improving both model stability and forecast accuracy.

## Use Case 

The resulting model provides a practical framework for analysing and forecasting regional housing markets across Australia. By quantifying how each region responds to national, mining, and lifestyle factors, the model enables identification of regions most exposed to structural trends, such as shifts in mining investment or lifestyle migration. These outputs can inform policy analysis, highlighting areas vulnerable to affordability pressures or cyclical volatility, and support investment decisions by revealing which regions are likely to outperform or lag under different macroeconomic conditions. 

## Datasets 

The datasets used in this project come primarily from the Australian Bureau of Statistics (ABS), covering housing price indexes and selected macroeconomic indicators. In addition, we make use of internally developed datasets, including preliminary outputs such as PCA results and estimated static coefficients. All datasets are organised and available in the data folder: 

* Housing indexes (log transformed) on both city level and SA4 level, named `city_indexes` and `sa4_indexes`
* PCA results including PC scores and loadings, named `df_pcs`, `df_eofs_city`, and `df_eofs_sa4`
* Factor proxies identified from PCA results, named `df_factor_trends`
* Regression coefficients estimated from the factor proxies, named `df_reg_coefs`
* Geographic data for SA4 regions, named `SA4_2021_AUST_SHP_GDA2020.shp`

## Project Limitation 

This project faces several dataset-related limitations, including but not limited to the following:

* Historical Scope: PCA results and coefficients are based on 29 years of data (1995–2024), which may not fully reflect future patterns.
* Measurement Bias: Indexes rely on hedonic and repeat-sales methods, which involve assumptions that may introduce bias.
* Proxy Validity: Factor proxies are heuristic choices and may not perfectly represent the underlying principal components.
* Data Sensitivity: Outcomes are highly data-driven and depend strongly on the chosen datasets and methods.
* Structural Shocks: Events like the mining boom, COVID-19, or policy shifts may not be well captured by static models or PCA.
* Macroeconomic Variables: The model does not explicitly incorporate broader economic indicators, which could enhance explanatory power.

# From Principal Components to Factor Proxies

As noted earlier, the PCA component of this work was conducted by Will Sijp as part of Sijp et al. (2025), and this project builds upon that foundation by constructing factor trends to serve as proxies for the principal components. The criterion for a good proxy is achieving a statistical correlation of around 96% with the original PC series.

To develop these proxies, we:

-   Analysed the PCA loadings to identify regions with the strongest positive and negative contributions to each component.
-   Formulated candidate proxies as linear combinations of housing indexes from these influential regions, focusing on those with clear economic or interpretive relevance.
-   Assessed the strength of each candidate by measuring its correlation with the corresponding principal component score.

To illustrate this process, we compared two candidate combinations for representing PC2 (the mining factor): the Brisbane–Sydney and Perth–Sydney spreads. By examining the shapes of the standardised series and testing their correlations with PC2, we found that the Perth–Sydney spread aligns more closely with the original component. 

Its scatterplot shows a tighter fit along the regression line, and the correlation with PC2 is marginally higher (98% versus 96% for the Brisbane–Sydney spread). Although the difference is small and not statistically significant, the Perth–Sydney spread provides the more robust proxy for PC2.

```{r examine-pc2-spreads, eval = FALSE}
# Compute the Perth Sydney Spread 
PS_raw <- city_indexes |> 
  select(month_date, greater_sydney, greater_perth)

U_raw <- factor_trends |> 
  select(month_date, market) 

com_PS <- PS_raw |> inner_join(U_raw, by = "month_date")

# Extract the numeric values and compute alpha 
mu_perth <- com_PS$`greater_perth`
mu_sydney <- com_PS$`greater_sydney`
U <- com_PS$market   

alpha <- cov(mu_perth, U, use = "complete.obs") /
         cov(mu_sydney, U, use = "complete.obs")

# Add in PS spread to factor_trend as mining_ps 
com_PS <- com_PS |> 
  mutate(mining_ps = `greater_perth` - alpha * `greater_sydney`) |> 
  select(month_date, mining_ps)

factor_trends <- factor_trends |> 
  inner_join(com_PS, by = "month_date")

# Join PC2 with both series 
Z2 <- pcs |> 
  filter(mode == 1) |>  
  select(period, pc_value)

mining_cmp <- factor_trends |> 
  select(month_date, mining, mining_ps) |>
  rename(period = month_date) |> 
  inner_join(Z2, by = "period") |> 
  mutate(
    z2_std = as.numeric(scale(pc_value)),
    deltaBS_std = as.numeric(scale(mining)),
    deltaPS_std = as.numeric(scale(mining_ps))
  )

# Plot PC2 with both series 
plot_ts <- mining_cmp |> 
  select(period, z2_std, deltaBS_std, deltaPS_std) |> 
  pivot_longer(-period, names_to = "series", values_to = "value") |> 
  mutate(series = recode(series,
                         z2_std = "PC2",
                         deltaBS_std = "δBS",
                         deltaPS_std = "δPS")) |> 
  ggplot(aes(x = as.Date(period), y = value, colour = series)) +
  geom_line() +
  labs(x = "Time", y = "Standardised value", colour = "Series",
       title = "PC2 vs δBS and δPS (standardised)", 
       subtitle = "Comparison of the spreads against the PCA-derived PC2 factor") +
  theme_minimal()

# Compare correlation 
corr_bs <- cor(mining_cmp$pc_value, mining_cmp$mining, use = "complete.obs")
corr_ps <- cor(mining_cmp$pc_value, mining_cmp$mining_ps, use = "complete.obs")

# Scatter plot 
scatter_df <- mining_cmp |> 
  select(pc_value, mining, mining_ps) |> 
  pivot_longer(cols = c(mining, mining_ps),
               names_to = "spread", values_to = "spread_value") |> 
  mutate(spread = recode(spread,
                         mining = glue("δBS (r = {round(corr_bs, 3)})"),
                         mining_ps = glue("δPS (r = {round(corr_ps, 3)})")))

plot_scatter <- ggplot(scatter_df, aes(x = pc_value, y = spread_value)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ spread, scales = "free_y") +
  labs(title = "Comparing PC2 Reconstructions from City Spreads", 
       subtitle = "Perth–Sydney spread (δPS) shows a closer alignment with PC2 than Brisbane–Sydney (δBS)", x = "PC2", y = "Spread value") +
  theme_minimal()

plot_scatter 
plot_ts
```

# Exploratory Analysis 

The exploratory analysis utilises visualisations to characterise the dynamics of the National trend and the Mining trend, thereby clarifying their underlying properties.

```{r autoplot}
# Time-series autoplot
p1 <- factor_trends |> autoplot(market) + labs(title = "National Sprend", x = "Month", y = "Index")
p2 <- factor_trends |> autoplot(mining) + labs(title = "Mining Spread", x = "Month", y = "Index")

p1 + p2 
```

**Time-series plots:** 

* U rises steadily from 2003 to 2012, transitions into a slower growth phase until 2019, then undergoes a sharp regime shift during 2020–2022 (the COVID period) followed by persistently elevated and volatile levels, with growth rates remaining time-varying throughout.

* δPS displays a prolonged, uneven rise from 2003 to 2012, followed by a sharp regime shift, then COVID-related fluctuations, with the level adjusting and showing time-varying patterns.

```{r season-plot}
# Seasonal plots
p3  <- factor_trends |> gg_season(market) + labs(title = "Seasonal Plot: Market", x = "Month", y = "Index")
p4 <- factor_trends |> gg_season(mining) + labs(title = "Seasonal Plot: Mining", x = "Month", y = "Index")
p3 + p4
```

**Seasonal plots:**

* U has apparent within-year slopes being dominated by the strong upward trend; no systematic month-of-year effects.

* δPS shows little evidence of month-to-month seasonality, with no particular month standing out, though its overall level varies between years.

```{r subseries-plot}
# Seasonal subseries
p5 <- factor_trends |> gg_subseries(market) + labs(title = "Seasonal Subseries: Market", x = "Month", y = "Index")
p6 <- factor_trends |> gg_subseries(mining) + labs(title = "Seasonal Subseries: Mining", x = "Month", y = "Index")
p5 + p6
```

**Seasonal sub-series plot:** 

* Each monthly panel of U shows a steady upward rise with mean value across months remaining similar, reinforcing that the series is driven more by trend than by seasonality.

* The δPS seasonal sub-series plot reinforces the lack of a dominant seasonal pattern, as the monthly profiles are similar in shape and the corresponding means remain relatively flat and close to zero.

# Forecasting with the Factor Model 

With the factors now established, we proceed to fit time series forecasting models to each series individually. As a first step, we examine the series in more detail. STL decomposition plots allow us to closely inspect the trend, seasonality, and remainder components, which helps determine the most suitable forecasting models for the series’ characteristics.

Stationary tests (ADF and KPSS) indicate that both the market index (U) and the Perth–Sydney spread (PS) are non-stationary, so differencing is required. However, based on economic intuition that the PS series may be mean-reverting, we applied the Zivot–Andrews (ur.za) test. This confirmed that PS could be stationary with several structural break.

The dataset was partitioned into training and testing subsets, with one year of observations allocated to the testing set. Given that the data are recorded at a monthly frequency and span only 25 years, this allocation represents an optimal balance between ensuring sufficient data for model training while preserving an adequate sample for evaluation.

```{r stationary-test, eval = FALSE}
# PS also I(1)
ur.df(factor_trends$mining, type="drift", lags=12) 
kpss.test(factor_trends$mining, null="Level")

# PS is stationary with a single structural break
ur.za(factor_trends$mining, model = "both") 
```

## National Trend 

The STL decomposition of the national trend U reveals a smooth and persistent upward trajectory in the trend component, while the seasonal component is of relatively minor magnitude. To stabilise variation in the seasonal component, a Box–Cox transformation was applied using the Guerrero method, which produced a λ value of 0.9. Given its proximity to one, the transformation had a negligible effect on the modelling outcome. The remainder component displays mostly small noise, with some bigger shocks.

```{r fitting-u-plot}
# Identify fits 
mkt_stl <- factor_trends |> 
  model(stl = STL(market))

mkt_stl |> 
  components() |> 
  autoplot() +
  labs(title = "STL Decomposition of Market Trend")

ggtsdisplay(factor_trends$market, main = "ACF and PACF with National Trend U")

# Split testing and training 
train_mkt <- factor_trends |> 
  select(market, month_date) |> 
  filter(month_date < yearmonth("2023 Jan")) 

test_mkt <- factor_trends |> 
  select(market, month_date) |> 
  filter(month_date >= yearmonth("2023 Jan")) 
```

The modelling strategy was guided by exploratory data analysis and the STL decomposition, which suggested a random walk with drift, `ARIMA(U ∼ pdq(0,1,0))`, as an appropriate baseline specification. 

To refine the model, we subsequently evaluated neighbouring specifications with alternative autoregressive and moving average terms, using AICc values and residual diagnostics as selection criteria. 

**Modelling process:**
In the modelling process, we evaluated a range of specifications informed both by diagnostic checks and prior knowledge. Inspection of the residuals suggested some semi-annual variation, and Fourier terms with `K = 2` were added to account for potential higher-frequency seasonality. We also investigated seasonal extensions with `P = 1`, alongside piecewise specifications with knots aligned to major economic events: the Global Financial Crisis (2008), the end of the mining investment boom (2012), and the onset of the COVID-19 pandemic (2020). These variations were assessed with respect to residual behaviour, statistical significance, and their ability to capture shocks apparent in the remainder component.

**Model diagnostics:**

* Model fit: Among the fitted models, two candidates emerged as particularly strong. The first, an ARIMA(1,1,0) with Fourier terms (K = 2) and drift. A second competitive specification combined Fourier terms (K = 2) with intervention dummies (step and pulse) and an ARIMA(2,1,1) structure. The table below presents the model fits, ranked according to their AICc values.

```{r fit-u-model}
# Function to add knots
add_pw <- function(df) {
  df |>
    mutate(
      ym = yearmonth(month_date),
      # step dummies
      step2008 = as.integer(ym >= yearmonth("2008 Jan")),
      step2012 = as.integer(ym >= yearmonth("2012 Jan")),
      step2020_03 = as.integer(ym >= yearmonth("2020 Mar")),
      # ramp: months since Mar 2020 
      ramp2020 = pmax(0L, as.integer(ym - yearmonth("2020 Mar"))),
      # pulse: only in Apr 2020
      pulse2020_04 = as.integer(ym == yearmonth("2020 Apr"))
    ) |>
    select(-ym)
}

train_mkt <- train_mkt |>
  as_tsibble(index = month_date) |>
  add_pw()

# Fit models 
fit_mkt <- train_mkt |> 
  model(
    autoarima = ARIMA(market),
    rw_drift  = ARIMA(market ~ 1 + pdq(0,1,0)),
    arima211  = ARIMA(market ~ 1 + fourier(K = 2) + pdq(2,1,1)),
    arima110  = ARIMA(market ~ 1 + fourier(K = 2) + pdq(1,1,0)),
    arima111  = ARIMA(market ~ 1 + fourier(K = 2) + pdq(1,1,1)),
    sarima211 = ARIMA(market ~ 1 + pdq(2,1,1) + PDQ(1,0,0)),
    piecewise = ARIMA(market ~ 1 + pdq(1,1,1) + step2008 + step2012 + ramp2020),
    step = ARIMA(market ~ fourier(K = 2) + step2020_03 + pulse2020_04 + pdq(2,1,1))
  ) 

# Examine fit  
glance(fit_mkt) |> arrange(AICc)
```

* Residual diagnostics: After adjusting for model-specific degrees of freedom, all models fall short on the Ljung-Box test, indicating residual autocorrelation to varying degrees. At this stage, we accept that some autocorrelation will remain because:
  
  * The models are built to capture broad structural patterns, not every subtle fluctuation.

  * Eliminating all autocorrelation risks overfitting and reduces generalizability.

  * Residual autocorrelation can be addressed more effectively in downstream modeling (e.g.,
  ARIMAX or multivariate regression).

We ultimately select **ARIMA(1,1,0)** as the preferred model because:

  * It is among models without intervention dummies and has the lowest AICc and the fewest
  parameters, enhancing model interpretability.
  
  * The model excludes seasonal terms, consistent with our earlier conclusion that
  seasonality is minor and difficult to interpret in this context.

  * The residual plot shows no strong or persistent autocorrelation, with most spikes within
  significance bounds and a roughly symmetric, zero-centered distribution.

```{r fit-u-model-residual-test}
# Residual diagnostics for best model
fit_mkt |> 
  select(arima110) |>
  gg_tsresiduals() + 
  ggtitle("Residual Diagnostics for ARIMA(1,1,0)",
          subtitle = "Model fit on Market factor shows mild autocorrelation")

fit_mktlong <- fit_mkt |>
  pivot_longer(cols = everything(), names_to = "model_name", values_to = "model_fit")

dof_lookup <- tibble(
  model_name = c(
    "autoarima",
    "rw_drift",
    "arima211",
    "arima110",
    "arima111",
    "sarima211",
    "piecewise",
    "step"
  ),
  dof = c(
    NA_integer_,  # autoarima depends on chosen (p,q)
    0,  # rw_drift
    7,  # arima211 (p+q=3, fourier=4)
    5,  # arima110 (p+q=1, fourier=4)
    6,  # arima111 (p+q=2, fourier=4)
    4,  # sarima211 (p+q+P+Q=4)
    5,  # piecewise (p+q=2, 3 exogenous dummies)
    9   # step (p+q=3, 6 exogenous regressors)
  )
)

# Extract model dof 
lb_tbl_mkt <- fit_mktlong |>
  left_join(dof_lookup, by = "model_name") |>
  mutate(
    lb_test = map2(model_fit, dof, ~{
      augment(.x) |>
        features(.innov, ljung_box, lag = 12, dof = .y)
    })
  ) |>
  unnest(lb_test) |>
  mutate(lb_pvalue = format(lb_pvalue, scientific = FALSE, digits = 2)) |> 
  select(model_name, lb_stat, lb_pvalue) |>
  arrange(desc(lb_pvalue)) 
lb_tbl_mkt

kable(lb_tbl_mkt, caption = "Ljung-Box Test Results for Model Residual on Market factor") 
```

## Mining Trend 

The mining trend (PS) oscillates around zero and appears mean-reverting with no evidence ofpermanent trend. Seasonality is present but relatively stable across years, while longer multi-year swings suggest the need for more flexible seasonal modelling. The residuals still display structure, with a clear AR(1) cut-off rather than pure white noise. 

```{r fit-ps-plot}
mining_stl <- factor_trends |> 
  model(stl = STL(mining))

mining_stl |> 
  components() |> 
  autoplot() +
  labs(title = "STL Decomposition of Mining Trend PS")

ggtsdisplay(factor_trends$mining, main = "ACF and PACF with Mining Trend PS")

# Split training and testing set 
train_mining <- factor_trends |>
  select(month_date, mining) |>
  filter(month_date < yearmonth("2023 Jan")) 

test_mining <- factor_trends |> 
  select(month_date, mining) |>
  filter(month_date >= yearmonth("2023 Jan")) 
```

**Modelling process:**

The mining trend (PS) captures the dynamics of the resource boom, which we interpret as a one-off structural break rather than a permanent driver. Prices surged sharply during the 2000s mining boom, collapsed around 2012, and have remained stagnant since. If this raw factor is carried forward into forecasts, the boom–bust cycle dominates and produces unstable predictions with excessively wide intervals.

To address this, we treat the boom as a temporary structural shock. We introduce a dummy variable (set to 1 during the boom/decline period and 0 otherwise) when modelling PS, and then forecast only the residual stationary component. This approach effectively assumes that another boom of comparable scale is unlikely.

The boom and decline phases were identified using the `breakpoints` function from the `strucchange` package, which detected three structural breaks in the series. The segment containing the global peak was classified as the “boom” period, followed by the “decline” period. These periods were then encoded into dummy variables for use in the modelling process, essentially fitting an ARIMA model to the residuals after accounting for the boom and decline periods. 

```{r detect-ps-break}
# Partition the series into boom and decline windows based on structural breaks
ps_zoo <- zoo(factor_trends$mining, order.by = factor_trends$month_date)
y  <- coredata(ps_zoo)
ti <- index(ps_zoo)
n  <- length(y)

# Three trend breaks 
bp_tr <- breakpoints(ps_zoo ~ time(ps_zoo), breaks = 3)

# Keep the breaks that actually exist 
idx <- bp_tr$breakpoints
idx <- idx[!is.na(idx)]

# Segment boundaries
cuts <- c(0, idx, n) 

# Locate the global peak and the segment that contains it
peak_i <- which.max(y)
seg_id <- findInterval(peak_i, cuts) 

# Boom = the whole peak segment, Decline = from end of peak segment to the next boundary
boom_start <- ti[cuts[seg_id]   + 1]
boom_end <- ti[cuts[seg_id+1]]
decline_start <- boom_end
decline_end <- ti[cuts[pmin(seg_id + 2, length(cuts))]]

# Create dummies
mining_df <- factor_trends |> 
  select(month_date, mining) |> 
  mutate(
    boom_dummy = as.integer(month_date >= boom_start & month_date <= boom_end),
    decline_dummy = as.integer(month_date > decline_start & month_date <= decline_end))


# Fitted piecewise trend for visual check
fit_tr <- fitted(bp_tr, breaks = length(idx))

# Plot 
ggplot(mining_df, aes(month_date, mining)) +
  geom_line(colour = "black") +
  geom_line(aes(y = as.numeric(fit_tr)), colour = "red", linewidth = 0.5) +
  geom_vline(xintercept = as.Date(ti[idx]), linetype = 2, colour = "blue") +
  annotate("rect", xmin = boom_start, xmax = boom_end, ymin = -Inf, ymax = Inf,
           alpha = 0.12, fill = "forestgreen") +
  annotate("rect", xmin = decline_start, xmax = decline_end, ymin = -Inf, ymax = Inf,
           alpha = 0.12, fill = "orange") +
  labs(
    title = "Mining Time Series of Boom and Decline Windows",
    subtitle = paste0(
      "Boom: ", boom_start, " \u2192 ", boom_end,
      "   |   Decline: ", decline_start, " \u2192 ", decline_end
    ),
    x = "Date", y = "Mining factor (Perth-Sydney Spread)"
  ) +
  theme_minimal()
```

```{r add-ps-dummy}
boom_start <- yearmonth(boom_start)
boom_end <- yearmonth(boom_end)
decline_start <- yearmonth(decline_start)
decline_end <- yearmonth(decline_end)

# Add in dummies for mining boom and decline periods
train_mining <- train_mining |> 
  mutate(
    boom = as.integer(month_date >= boom_start & month_date <= boom_end),
    decline = as.integer(month_date >= decline_start & month_date <= decline_end)
  )
```

* Model fit: We fitted several ARIMA specifications for the mining factor, using the boom and decline dummies as exogenous regressors. The candidate models were chosen based on visual inspection of the ACF/PACF plots and general knowledge of the factor’s cyclical, mean-reverting nature. Among all the models fitted, only the ARIMA (2,0,1) model converged successfully and produced well-behaved residuals, for three main reasons:

  * Simpler (AR(1) or AR(2)) models tended to violate stationarity, while including the MA(1) term in ARIMA (2,0,1) stabilised the process and allowed the AR coefficients to remain within the stationary region.

  * The MA(1) component effectively captured short-term fluctuations that the boom and decline dummies could not explain, preventing the residuals from showing remaining autocorrelation.

  * Higher-order or alternative specifications failed to converge or produced autocorrelated residuals.

* Residual diagnostics: The residuals are well-behaved, fluctuating randomly around zero with no autocorrelation, comfirmed by ljung-box test. Their near-normal distribution suggests the model captures the mining factor’s dynamics effectively.

```{r fit-ps-model}
# Fit ARIMA with residuals after accounting for the dummies 
fit_mining <- train_mining |>
  model(
    arima201 = ARIMA(mining ~ boom + decline + pdq(2,0,1)), 
    arima100 = ARIMA(mining ~ boom + decline + pdq(1,0,0)),
    arima200 = ARIMA(mining ~ boom + decline + pdq(2,0,0)),
    arima101 = ARIMA(mining ~ boom + decline + pdq(1,0,1))
  ) |> 
  select(arima201)

glance(fit_mining) |> arrange(AICc) 

fit_mining |> 
  select(arima201) |> 
  gg_tsresiduals() + 
  ggtitle("Residual Diagnostics for ARIMA(2,0,1)",
          subtitle = "Model fit on Mining factor shows no autocorrelation")

lb_tbl_mining <- fit_mining |>
  augment() |>
  features(.innov, ljung_box, lag = 12, dof = 3)

kable(lb_tbl_mining, caption = "Ljung-Box Test Results for Model Residual on Mining factor") 
```

## Forecast Performance 

For the forecasting stage, we use the best-performing models (ARIMA(1,1,0) with Fourier terms (K = 2) for U and ARIMA(2,0,1) for PS) identified earlier to project values 10 years ahead (120 months). We generate forecasts with prediction intervals at both 80% and 95%. These intervals show the range in which future values are likely to fall, giving us a measure of uncertainty, an 80% interval means there is an 8 in 10 chance that the true value will lie within that range.

For evaluation, we set aside one year of data (2023) as a test set. Although a typical split is about 20% of the dataset, our series only spans 29 years, so using 20% would cut off too much valuable information for forecasting. A one-year test period strikes a better balance.

The forecasts indicate that the market trend is expected to rise, with an 80% prediction interval width of 0.9350, reflecting higher uncertainty. In contrast, the mining trend shows a flatter path, with an 80% interval width of 0.6107. This narrower range reflects greater stability, largely because we excluded the one-off boom and decline period, which would otherwise make the series more volatile.

```{r prediction-interval}
# Forecast - 10 years
h <- 120
fit_mkt <- fit_mkt |> select(arima110)
last_idx <- max(train_mining$month_date)  
future_mining <- tibble(
  month_date = last_idx + (1:h),
  boom = 0L,
  decline = 0L
) |> 
  as_tsibble(index = month_date)

fc_mkt  <- fabletools::forecast(fit_mkt,  h = h, level = c(80, 95))
fc_mining <- fabletools::forecast(fit_mining, new_data = future_mining , h = h, level = c(80, 95))

# Produce fan with for U 
fcbands_mkt <- fc_mkt |> 
  as_tibble() |> 
  mutate(
    lo80 = quantile(market, 0.10),
    hi80 = quantile(market, 0.90),
    lo95 = quantile(market, 0.025),
    hi95 = quantile(market, 0.975),
    width80 = hi80 - lo80,
    width95 = hi95 - lo95
  ) |> 
  select(.model, month_date, mean = `.mean`, lo80, hi80, lo95, hi95, width80, width95) 

last_row_mkt <- fcbands_mkt |> filter(month_date == max(month_date))

# Forecast plot with fan width at last obs 
autoplot(
  fc_mkt,
  data = factor_trends |> 
    filter(month_date <= max(train_mkt$month_date))
) +
  geom_segment(
    data = last_row_mkt,
    aes(x = month_date, xend = month_date, y = lo80, yend = hi80, colour = .model),
    linewidth = 0.8
  ) +
  geom_text(
    data = last_row_mkt,
    aes(x = month_date, y = hi80, label = sprintf("w80 = %.4f", width80), colour = .model),
    vjust = -0.6, size = 2.5
  ) +
  labs(
    title = "Forecasts of Market Trend with 80% Prediction Interval",
    subtitle = "The forecasts start after the training window and show widening uncertainty bands"
  )

# Produce fan width for mining 
fcbands_mining <- fc_mining  |> 
  as_tibble() |> 
  mutate(
    lo80 = quantile(mining, 0.10),
    hi80 = quantile(mining, 0.90),
    lo95 = quantile(mining, 0.025),
    hi95 = quantile(mining, 0.975),
    width80 = hi80 - lo80,
    width95 = hi95 - lo95
  ) |> 
  select(.model, month_date, mean = `.mean`, lo80, hi80, lo95, hi95, width80, width95) 

last_row_mining <- fcbands_mining |> filter(month_date == max(month_date))

autoplot(fc_mining, 
         data = factor_trends |> 
           filter(month_date <= max(train_mining$month_date))) +
  geom_segment(data = last_row_mining,
               aes(x = month_date, xend = month_date, y = lo80, yend = hi80, colour = .model),
               linewidth = 0.8) +
  geom_text(data = last_row_mining,
            aes(x = month_date, y = hi80,
                label = sprintf("w80 = %.4f", width80),
                colour = .model),
            vjust = -0.6, size = 2.5) + 
  labs(title = "Forecasts of Mining Trend with 80% Prediction Interval",
       subtitle = "Excluding the boom/decline period produces more stable forecasts")
```

The test results show that the chosen model for the market trend performs well, with low error (RMSE = 0.056, small relative to the series scale). For the mining trend, the model with boom and decline dummies yields forecasts with an average error of about 7%, which is reasonable given the mining factor’s higher volatility and cyclical nature. Despite minor residual autocorrelation, both models perform robustly overall.

```{r test-set-performance}
acc_mkt <- accuracy(fc_mkt, test_mkt) |>
  select(.model, .type, RMSE, MAE)

acc_mining <- accuracy(fc_mining, test_mining) |>
  select(.model, .type, RMSE, MAE)

# Add labels
acc_mkt  <- acc_mkt  |> mutate(series = "Market")
acc_mining <- acc_mining |> mutate(series = "Mining")

# Combine tables 
acc_all <- bind_rows(acc_mkt, acc_mining) |>
  relocate(series, .model, .type) |> 
  print()
```

```{r save-forecast-df}
# Convert forecasts to tibble
df_fc_mkt  <- as_tibble(fcbands_mkt)
df_fc_mining <- as_tibble(fcbands_mining)

df_fc <- df_fc_mkt |>
  select(month_date, mean_mkt = mean, lo80_mkt = lo80, hi80_mkt = hi80,
         lo95_mkt = lo95, hi95_mkt = hi95) |>
  left_join(
    df_fc_mining |>
      select(month_date, mean_mining = mean, lo80_mining = lo80, hi80_mining = hi80,
         lo95_mining = lo95, hi95_mining = hi95),
    by = "month_date"
  )

write_csv(df_fc, "data/df_forecasts.csv")
```


# Factor Model Application 

```{r ols-reg}
# Run regression 
fit_reg_table <- function(idx_long, region_col, region_level_label) {
  stopifnot(all(c("month_date", region_col, "value") %in% names(idx_long)))
  
  idx_long |>
    inner_join(factor_trends, by = "month_date") |>
    group_by(.data[[region_col]]) |>
    group_modify(\(.x, .y) {
      .x <- tidyr::drop_na(.x, value, market, mining, lifestyle)
      if (nrow(.x) < 10) return(tibble(
        alpha = NA_real_, market = NA_real_, mining = NA_real_,
        lifestyle = NA_real_, r2 = NA_real_, durbin_watson = NA_real_
      ))
      fit <- lm(value ~ market + mining + lifestyle, data = .x)
      coefs <- coef(fit)
      r2    <- summary(fit)$r.squared
      dw    <- tryCatch(as.numeric(lmtest::dwtest(fit)$statistic[["DW"]]),
                        error = \(e) NA_real_)
      tibble(
        alpha     = unname(coefs[["(Intercept)"]]),
        market    = unname(coefs[["market"]]),
        mining    = unname(coefs[["mining"]]),
        lifestyle = unname(coefs[["lifestyle"]]),
        r2        = r2,
        durbin_watson = dw
      )
    }) |>
    ungroup() |>
    rename(region = !!region_col) |>
    mutate(region_level = region_level_label) |>
    select(region, alpha, r2, durbin_watson, market, mining, lifestyle, region_level)
}

# Prepare city indexes
city_long <- city_indexes |>
  pivot_longer(
    cols = -month_date,
    names_to = "city",
    values_to = "value"
  )

# Prepare SA4 indexes
 sa4_long <- sa4_indexes |>
   pivot_longer(
     cols = -month_date,
     names_to = "region",
     values_to = "value")

# Build regression coefficient table 
reg_coefs_city <- fit_reg_table(city_long, region_col = "city",  region_level_label = "major_city")
reg_coefs_sa4  <- fit_reg_table(sa4_long,  region_col = "region", region_level_label = "sa4_name")

reg_coefs_new <- bind_rows(reg_coefs_city, reg_coefs_sa4)

readr::write_csv(reg_coefs_new, "data/df_reg_coefs.csv")
```


## Goodness of Fit

We now turn to evaluating how well the factor model captures the variation in individual major city and SA4 indexes. R² measures the share of variation in each major city or SA4 housing index that our factor model explains. 

**For major cities and rest-of-state:** Most city and regions have an R² value above 0.98. The extremes highlight clear spatial patterns:

* Top cities and rest-of-state: Large east-coast capitals and their surrounding belts.

* Bottom cities and rest-of-state: Smaller or resource-dependent markets.

```{r model-r2-city}
reg_city <- reg_coefs_new |> filter(region_level == "major_city")

# Rank the major cities by r2 and print top and bottom 10 
ranked_city <- reg_city |>
  arrange(desc(r2)) |>
  mutate(rank = row_number())

top10_city <- ranked_city |>
  slice_head(n = 5) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

bottom10_city <- ranked_city |>
  slice_tail(n = 5) |>
  arrange(r2) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

tbl_rank_city <- top10_city |>
  full_join(bottom10_city, by = "row", suffix = c("_Top", "_Bottom")) |>
  select(Rank_Top, Region_Top, R2_Top, Rank_Bottom, Region_Bottom, R2_Bottom)

kable(
  tbl_rank_city,
  caption = "Top 5 and Bottom 5 Major City and Area by R²",
  digits = 4,
  col.names = c("Rank", "Region", "R\u00B2", "Rank", "Region", "R\u00B2"),
  escape = FALSE
) |>
  kable_styling(full_width = TRUE,
                bootstrap_options = c("striped", "condensed")) |>
  add_header_above(c("Top 5" = 3, "Bottom 5" = 3)) |>
  column_spec(1, width = "4em") |>
  column_spec(4, width = "4em") |>
  column_spec(3, width = "6em") |>
  column_spec(6, width = "6em")
```

**For SA4 regions**: The SA4 regions shares the same stories. R² is tightly concentrated near 1.0 with the majority of SA4s exceed 0.98, with only a small left tail (a few around 0.92 – 0.95). The top and bottom lists identify the extremes and show recurring patterns across those regions: 

* Top 10 regions: Concentrated in the large east-coast capitals and surrounding belts.
* Bottom 10 regions : Dominated by Western Australia and Queensland’s mining exposed regions.

```{r model-r2-sa4}
# Filter the SA4 regions in the coefficient data 
reg_sa4 <- reg_coefs_new |> filter(region_level == "sa4_name")

# Rank the regions by r2 and print top and bottom 10 
ranked_sa4 <- reg_sa4 |>
  arrange(desc(r2)) |>
  mutate(rank = row_number())

top10_sa4 <- ranked_sa4 |>
  slice_head(n = 10) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

bottom10_sa4 <- ranked_sa4 |>
  slice_tail(n = 10) |>
  arrange(r2) |>
  transmute(row = row_number(), Rank = rank, Region = region, R2 = r2)

tbl_rank_sa4 <- top10_sa4 |>
  full_join(bottom10_sa4, by = "row", suffix = c("_Top", "_Bottom")) |>
  select(Rank_Top, Region_Top, R2_Top, Rank_Bottom, Region_Bottom, R2_Bottom)

kable(
  tbl_rank_sa4,
  caption = "Top 10 and Bottom 10 SA4 Regions by R²",
  digits = 4,
  col.names = c("Rank", "Region", "R\u00B2", "Rank", "Region", "R\u00B2"),
  escape = FALSE
) |>
  kable_styling(full_width = TRUE,
                bootstrap_options = c("striped", "condensed")) |>
  add_header_above(c("Top 10" = 3, "Bottom 10" = 3)) |>
  column_spec(1, width = "4em") |>
  column_spec(4, width = "4em") |>
  column_spec(3, width = "6em") |>
  column_spec(6, width = "6em")
```

Both SA4 regions and major cities display highly concentrated R² distributions with major cities showing less variation.

```{r model-r2-comp}
dist_comp <- bind_rows(
  reg_sa4 |> transmute(group = "SA4 (n=88)", r2),
  reg_city |> transmute(group = "Major cities (n=15)", r2)
)
ggplot(dist_comp, aes(x = r2, y = group)) +
  ggdist::stat_slab(aes(thickness = after_stat(pdf)),
                    adjust = 1,
                    alpha = 0.5) +
  ggdist::stat_pointinterval(.width = c(0.5, 0.8, 0.95), point_size = 1.8) +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(title = "Factor Model R² distribution: SA4 vs major cities", x = "R²", y = NULL) +
  theme_minimal(base_size = 12)
```

The choropleth reinforces the findings from the summary table, as we can see that the east-coast capitals and coastal belts from Melbourne through Sydney into the South-East of Queensland show the strongest fit,  with most regional NSW/VIC also high. While fit weakens inland and to the west, notably across Western Australia and along Queensland’s mining belts; The dark purple areas mark the lowest R² and are typically remote and resource-exposed regions with possible boom–bust timing and sparse transactions that our three factors don’t fully capture.

Hence, we’ll prioritise Western Australia and Queensland’s mining-exposed regions for residual diagnostics, where we expect autocorrelation and regime shifts for trial enhancements. 

```{r model-r2-dist-plot}
# Match names between reg_sa4 and sa4 shapefile
norm <- function(x) {
  x |>
    str_to_upper() |>
    str_replace_all("&", "AND") |>
    str_replace_all("[[:punct:]]", " ") |>
    str_squish()
}

# Filter city and geo coord 
major_cities <- c(
  "AUSTRALIAN CAPITAL TERRITORY",
  "GREATER ADELAIDE",
  "GREATER BRISBANE",
  "GREATER DARWIN",
  "GREATER HOBART",
  "GREATER MELBOURNE",
  "GREATER PERTH",
  "GREATER SYDNEY"
)

city_r2 <- reg_coefs_new |>
  filter(region %in% major_cities) |>
  transmute(city = region, r2)

city_coords <- tibble::tribble(
  ~city, ~lon, ~lat,
  "GREATER SYDNEY", 151.21, -33.87,
  "GREATER MELBOURNE", 144.96, -37.81,
  "GREATER BRISBANE", 153.03, -27.47,
  "GREATER PERTH", 115.86, -31.95,
  "GREATER ADELAIDE", 138.60, -34.93,
  "GREATER HOBART", 147.33, -42.88,
  "GREATER DARWIN", 130.84, -12.46,
  "AUSTRALIAN CAPITAL TERRITORY", 149.13, -35.28
)

city_mapdata <- city_r2 |> left_join(city_coords, by = "city")

# Prepare the 88 regions 
keep88 <- reg_sa4 |>
  mutate(region_norm = norm(region)) |>
  distinct(region_norm)

# Clean shapefile + keep regions + join r2 
sa4_r2 <- sa4 |>
  mutate(region_norm = norm(SA4_NAME21)) |>
  # drop empty geometries and special APS areas
  filter(!st_is_empty(geometry)) |>
  filter(!str_detect(region_norm, "^MIGRATORY\\s+OFFSHORE\\s+SHIPPING"),
         !str_detect(region_norm, "^NO\\s+USUAL\\s+ADDRESS")) |>
 semi_join(keep88, by = "region_norm") |> # Keep only the 88 sa4
  left_join( # Join r2 
    reg_sa4 |>
      mutate(region_norm = norm(region)) |>
      select(region_norm, r2),
    by = "region_norm"
  )

# Plot the r2 distribution 
ggplot() +
  geom_sf(data = sa4_r2, aes(fill = r2), linewidth = 0.1, colour = "grey70") +
  scale_fill_viridis_c(name = "SA4 R²", labels = percent) +
  geom_point(data = city_mapdata, aes(x = lon, y = lat), colour = "red", size = 1) +
  ggrepel::geom_label_repel(
  data = city_mapdata,
  aes(x = lon, y = lat, label = paste0(city, "\n", scales::percent(r2, 0.01))),
  size = 1.5, label.padding = unit(0.05, "lines"), label.r = unit(0.05, "lines"),    
  label.size = 0.15, box.padding = 0.2, seed = 42) +
  coord_sf() +
  labs(title = "Factor Model R² Distribution across Cities & Regions") +
  theme_void(base_size = 12)
```

## Residual Autocorrelation

At this stage, the model does not incorporate any time dynamics in the residuals. While it explains a substantial share of the variance for both cities and regions, the trending and autocorrelated nature of housing indexes means that the residuals themselves display extremely strong positive autocorrelation. This is evident in the Durbin–Watson statistics, which are close to zero across all regions: Major cities cluster around 0.02 to 0.05, while SA4 regions have a slightly broader range, extending up to 0.2, but still remain far below the benchmark value of 2. This pattern underscores the limitations of using a static regression framework without accounting for time-series dynamics.

```{r dw-stat}
reg_coefs_new |>
  group_by(region_level) |> 
  summarise(
    dw_min = min(durbin_watson),
    dw_max = max(durbin_watson),
    dw_mean = mean(durbin_watson),
    dw_median = median(durbin_watson), 
    range = max(durbin_watson) - min(durbin_watson)
  )

ggplot(reg_coefs_new, aes(x = region_level, y = durbin_watson, fill = region_level)) +
  geom_violin(trim = FALSE, alpha = 0.6) +
  geom_boxplot(width = 0.1, outlier.shape = NA) +
  labs(title = "Durbin–Watson by region type",
       x = "", y = "Durbin–Watson") +
  coord_cartesian(ylim = c(0, 0.25)) + 
  theme_linedraw()
```

## Coefficient Pattern

While R² and Durbin–Watson assess overall fit and residual behaviour, they do not show how regions load onto each factor. Examining coefficient patterns adds this context, revealing whether markets move uniformly with the national trend or diverge due to mining influences.

The distribution of market coefficients is tightly centred around 1.0 with relatively little variation. This indicates that most regions respond in a similar way to the national housing trend. In contrast, the mining coefficients are far more dispersed with a long right tail and several extreme outliers, suggesting that exposure to mining-related dynamics differs substantially across regions.

The scatterplot analysis reveals a clear negative relationship between the market and mining coefficients. Regions with a strong loading on the national market factor tend to exhibit weaker or even negative loadings on the mining factor and vice versa.

```{r coef-pattern}
reg_coef_long <- reg_coefs_new |>
  select(region_level, market, mining) |>
  pivot_longer(c(market, mining),
               names_to = "factor", values_to = "coef")

# Density histogram 
ggplot(reg_coef_long, aes(x = coef, fill = factor)) +
  geom_histogram(aes(y = after_stat(density)),
                 bins = 30, , colour = "white") +
  geom_density(size = 0.5, fill = NA) +
  facet_wrap(~factor, scales = "free_x") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "Coefficient", y = "Density",
       title = "Distribution of Estimated Coefficient", 
       subtitle = "Market coefficients are tightly clustered around 1.0, while mining coefficients show greater dispersion")

# Pairwise
cols <- c("market","mining")
ggpairs(
  reg_coefs,
  columns = match(cols, names(reg_coefs)),
  mapping = aes(colour = region_level),
  upper = list(continuous = wrap("cor", size = 3)),
  lower = list(continuous = wrap("smooth_loess", alpha = 0.6, size = 0.3)),
  diag  = list(continuous = wrap("densityDiag", alpha = 0.6))
) + 
  labs(title = "Relationship Between Market and Mining Coefficients",
       subtitle = "Regions more sensitive to the national housing trend generally show lower sensitivity to mining dynamics") 
```

# Model Enhancement 

From the analysis of the OLS factor model, several limitations were identified:

-   Strong residual autocorrelation, reflecting the trending and persistent nature of housing price movements.

-   Spatial variation in model fit, with mining-exposed regions showing weaker R² values and more dispersed coefficient estimates.

-   Lack of temporal adaptability, as static coefficients cannot capture evolving relationships between regional markets and national or sectoral factors.

To address these issues, two key enhancements were introduced:

-   ARIMAX-based residual control, incorporating autoregressive and moving-average terms to explicitly model autocorrelation and stabilise residuals.

-   Time-varying estimation using expanding windows, allowing coefficients to evolve with changing market conditions and improving responsiveness to structural shifts in housing dynamics.

## Model Enhancement I: ARIMAX-Based Residual Control

The ARIMAX framework maintains fixed regression coefficients while addressing serial dependence in the residuals through an ARIMA process. In this context, the differencing order is constrained to zero to ensure that the analysis captures long-term co-movements between the dependent variable y and the underlying factor trends, rather than focusing on short-term fluctuations.

Several baseline specifications were tested through a grid search for the orders, and the optimal model was chosen by balancing AICc performance with Ljung–Box test results, ensuring that the residuals approximated white noise.

As a result, five ARIMAX models were selected and fitted across regions according to their individual time series dynamics. Among the 88 SA4 regions, 76 passed the Ljung–Box test, indicating that their residuals approximate white noise.

```{r reg-arimax-search}
# Define variables 
x_df <- factor_trends |> select(month_date, market, mining, lifestyle)

df_all <-  sa4_long |>
  inner_join(x_df, by = "month_date") |>
  arrange(region, month_date) |>
  drop_na()

# Candidates for ARIMAX orders 
cands <- list(
  c(0,0,0), c(1,0,0), c(0,0,1), c(1,0,1),
  c(2,0,0), c(0,0,2), c(2,0,1), c(1,0,2), c(2,0,2)
)

# Fit per region, ranking by AIC and LB
results <- df_all |>
  group_by(region) |>
  reframe({
    dates <- month_date
    y <- value
    X <- as.matrix(pick(market, mining, lifestyle))
    y_ts <- ts(y, frequency = 12, start = c(year(min(dates)), month(min(dates))))
    n <- length(y)
   # placeholders 
    best <- NULL
    best_AICc <- Inf
    best_p <- NA
    best_fit <- NULL
    # fit on regions 
    for (od in cands) {
      fit <- try(Arima(y_ts, order = od, xreg = X, include.mean = TRUE, method = "ML"), silent = TRUE)
      if (inherits(fit, "try-error")) next
      k <- length(coef(fit))
      aic <- AIC(fit)
      aicc <- aic + (2 * k * (k + 1)) / pmax(n - k - 1, 1)
      pval <- as.numeric(Box.test(residuals(fit), lag = 12, type = "Ljung-Box")$p.value)

      if (aicc < best_AICc || (aicc == best_AICc && pval > best_p)) {
        best_AICc <- aicc
        best_p <- pval
        best_fit <- fit
        best <- paste(od, collapse = ",")
      }
    }
    if (is.null(best_fit)) return(NULL)
    b <- coef(best_fit)[c("market", "mining", "lifestyle")]
    tibble(
      best_model = paste0("ARIMAX(", best, ") d=0"),
      AICc = best_AICc,
      LB_p12 = best_p, #LB at lag 12 
      market = unname(b[1]),
      mining = unname(b[2]),
      lifestyle = unname(b[3])
    )
  })

results |> filter(LB_p12 >=0.05) |> count()
results |> group_by(best_model) |> summarise(n = n(), pass_rate = mean(LB_p12 >= 0.05)) |> arrange(desc(pass_rate))

reg_coefs_arimax <- results |> select(region, market, mining, lifestyle)
```

This section compares the coefficient distributions for three factors across two modelling approaches: OLS and ARIMAX. The OLS model (blue) represents a static regression using contemporaneous factors only, while the ARIMAX model (red) incorporates time-series autocorrelation to account for temporal dependencies in the residuals. From the boxplot we can see that: 

* Market Factor: Both models yield strong positive coefficients with tightly clustered distributions. The medians are nearly identical, and the IQRs are narrow. This consistency confirms that national market trends exert a robust and uniform influence on regional price movements, regardless of the modeling framework.

* Mining Factor: Median coefficients are close to zero in both models. The OLS model shows a wider spread, including more positive outliers, while ARIMAX produces smaller and more stable estimates. This suggests that once temporal persistence is accounted for, the mining signal is dampened but still present, reflecting a more conservative attribution of its influence.

* Lifestyle Factor: Both models show similar distributions, with medians around 0.3 and comparable IQRs. ARIMAX coefficients are slightly more tightly clustered, indicating a slight attenuation of the lifestyle signal when autocorrelation is modelled. This implies that lifestyle effects are modest and less regionally variable once temporal dynamics are considered.

To illustrate this, we compare factor coefficients for Melbourne and Sydney’s inner areas, along with Blacktown and the Central Coast, which are regions influenced by the mining and lifestyle factors. The ARIMAX model provides a more reliable fit by addressing autocorrelation, resulting in more stable and realistic factor sensitivities across regions. Overall, when time dependence is modelled with ARIMAX, the market sensitivity increases across all regions, while the absolute influence of the mining and lifestyle factors weakens: 

* Market Factor: Each region becomes more tightly linked to the national housing cycle once autocorrelation is controlled for. For example, the Central Coast rises from 0.94 to 1.07, and Sydney–Inner South from 1.06 to 1.08.

* Mining Factor: The negative mining effect softens everywhere (e.g., Sydney–Blacktown from −0.52 to −0.36), implying mining trend of the OLS came from persistent shocks rather than true long-run sensitivity.

* Lifestyle Factor: The lifestyle effect also becomes less extreme, most notably in the Central Coast, where it drops slightly but remains positive, confirming its coastal lifestyle character, while capital and suburban areas stay negative.

```{r coef-compare}
# Compare coefficients: OLS & ARIMAX 
reg_coefs_ols <- reg_coefs_new |> mutate(model_type = "OLS")
reg_coefs_arimax <- reg_coefs_arimax |> mutate(model_type = "ARIMAX")

reg_compare <- bind_rows(reg_coefs_ols, reg_coefs_arimax) |> 
  select(region, model_type, market, mining, lifestyle) 

# Save coefficient comparison data frame 
write_csv(reg_compare, "data/df_coef_compare.csv")

reg_compare |>
  pivot_longer(cols = c(market, mining, lifestyle),
               names_to = "factor",
               values_to = "estimate") |>
  ggplot(aes(x = model_type, y = estimate, fill = model_type)) +
  geom_boxplot(alpha = 0.7, outlier.alpha = 0.2) +
  facet_wrap(~factor, scales = "free_y") +
  labs(
    title = "Coefficient Comparison: OLS vs ARIMAX",
    x = NULL,
    y = "Coefficient Value"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

# Example: Melbourne vs. Sydney 
reg_subset <- reg_compare |>
  filter(region %in% c(
    "MELBOURNE - INNER", 
    "SYDNEY - CITY AND INNER SOUTH",
    "SYDNEY - BLACKTOWN", 
    "CENTRAL COAST"
  )) |> 
  arrange(region)

kable(reg_subset, 
      caption = "Factor Coefficient Comparison: OLS vs ARIMAX for Selected Regions")
```

## Model Enhancement II: Time-dependent Coefficients 

The expanding window approach was used to assess the stability of model coefficients over time. By gradually extending the estimation sample, we tested whether the relationships between regional housing indices and key factors remained consistent as new data of the following years was added. We refitted the OLS regression twice: First using data up to November 2022, and again including data up to November 2024. Comparing these two sets of coefficients allowed us to evaluate how much the estimated sensitivities of each factor changed when the sample expanded by two years.

The table results displays the average yearly change and standard deviation of coefficients across regions. The intercept and market factors show small but positive shifts, while mining and lifestyle remain nearly unchanged. This suggests that the overall influence of these factors on housing prices has been highly stable between 2022 and 2024. While whe scatter plot compares the 2022 and 2024 coefficients for each factor. Points closely follow the 45° line, confirming that the factor effects are nearly identical across years. 

In conclusion, adding the most recent year of data did not materially alter the model’s relationships, supporting the robustness of the factor structure.

```{r expending-window}
# Configuration 
target <- "value"
predictors <- c("market","mining","lifestyle")
panel <- sa4_long |>
  inner_join(factor_trends, by = "month_date") |> 
  arrange(region, month_date) |> 
  drop_na(all_of(c(target, predictors)))

# Set up cutoff dates - two years apart 
start_date <- as.Date("1995-02-01")
end_date   <- as.Date("2024-11-01")
cutoffs    <- as.Date(c("2022-11-01", "2024-11-01")) 

# Refit ols for expanding window 
fit_ols <- function(y, X) {
  Xs <- scale(as.matrix(X))
  co <- coef(lm(y ~ Xs))
  tibble(
    intercept = unname(co[1]),
    !!!setNames(as.list(unname(co[-1])), predictors)
  )
}

# Build window 
expand_to <- function(df, cut) {
  df_win <- df |>
    filter(month_date >= start_date, month_date <= cut)
  fit_ols(
    y = df_win[[target]],
    X = df_win[, predictors, drop = FALSE]
  ) |>
    mutate(till_date = cut)
}

# Results 
coefs_cuts <- panel |>
  filter(month_date >= start_date, month_date <= end_date) |>
  group_by(region) |>
  group_modify(~{
    reg <- .x
    purrr::map_dfr(cutoffs, ~ expand_to(reg, .x))
  }) |>
  ungroup()

# Comparison - table and plot 
coef_diff <- coefs_cuts |>
  pivot_longer(
    cols = c(intercept, market, mining, lifestyle),
    names_to = "factor", values_to = "beta"
  ) |>
  pivot_wider(
    names_from = till_date, 
    values_from = beta
  ) |> 
  rename(as_of_2022 = `2022-11-01`, as_of_2024 = `2024-11-01`) |>
  mutate(diff = as_of_2024 - as_of_2022) 

coef_diff_tbl <- coef_diff |>
  group_by(factor) |>
  summarise(mean_change = mean(diff), sd_change = sd(diff)) 
  
kable(coef_diff_tbl, 
      caption = "Mean and Standard Deviation of Coefficient Changes (2022-2024)", digits = 4)

ggplot(coef_diff, aes(x = as_of_2024, y = as_of_2022, colour = factor)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  geom_point(alpha = 0.8) +
  facet_wrap(~factor, scales = "free") +
  labs(
    title = "Coefficient Stability: 2022 vs 2024",
    subtitle = "Coefficients show strong year-to-year stability across regions",
    x = "Coefficient (up to 2022-11)",
    y = "Coefficient (up to 2024-11)"
  ) +
  theme_minimal()
```

# Assessing the Boundedness of Market-Driven Forecasts

To evaluate the reliability and stability of the regional housing forecasts, we assessed how bounded the projections are when driven solely by the national market factor. 

The analysis was conducted using coefficients derived from both the OLS and ARIMAX specifications. While the OLS model provides a static benchmark, the ARIMAX model extends it by accounting for temporal dependence and reducing residual autocorrelation. 

In both cases, the mining factor spread was treated as the only varying source of uncertainty, with all other factors held constant. For each region, the 10-year growth implied by the market factor was projected, and a 95 per cent multiplicative uncertainty band was computed based on the volatility of the mining factor and the residual variance from each model. 

